<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
        <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <!--
    <meta name="description" content="">
    <meta name="author" content="">
    -->
    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="https://minglunwu.github.io/theme/css/base.css">
    <!--<link rel="stylesheet" href="https://minglunwu.github.io/theme/css/vendor.css">-->
    <link rel="stylesheet" href="https://minglunwu.github.io/theme/css/main.css">
    <link rel="stylesheet" href="https://minglunwu.github.io/theme/css/styles.css">

    <!-- script
    ================================================== -->
    <script src="https://minglunwu.github.io/theme/js/modernizr.js"></script>
    <script src="https://minglunwu.github.io/theme/js/pace.min.js"></script>
    <!--<script src="https://kit.fontawesome.com/968a4ded4c.js" crossorigin="anonymous"></script>-->

    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="https://minglunwu.github.io/theme/favicon.ico" type="image/x-icon">
    <link rel="icon" href="https://minglunwu.github.io/theme/favicon.ico" type="image/x-icon">

    <title>NLPæ·±åº¦å­¸ç¿’ä¸èƒ½ä¸ç”¨çš„å¥—ä»¶ - Transformers</title>
    <meta property="og:title" content="NLPæ·±åº¦å­¸ç¿’ä¸èƒ½ä¸ç”¨çš„å¥—ä»¶ - Transformers">
    <meta name="description" content="<p>é€éTransformerså¥—ä»¶ä½¿ç”¨å¤šç¨®Pre-trained Language Modelï¼Œä¸¦ä¸”å¯¦ä½œText Feature Extraction.</p>">
    <meta property="og:description" content="<p>é€éTransformerså¥—ä»¶ä½¿ç”¨å¤šç¨®Pre-trained Language Modelï¼Œä¸¦ä¸”å¯¦ä½œText Feature Extraction.</p>">
    <meta name="author" content="MingLun Allen Wu">
    <meta property="og:image" content="https://images.unsplash.com/photo-1554631995-6c22c6e729fe?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=733&q=80">
    
    <link rel="image_src" type="image/jpeg" href="https://images.unsplash.com/photo-1554631995-6c22c6e729fe?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=733&q=80" />
    <style>
        img{
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    </style>
</head>

<body id="top">
            <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="https://minglunwu.github.io"><img src="https://minglunwu.github.io/theme/images/logo.png" width="160" height="56" alt="Homepage"></a>
        </div>

        <nav class="header-nav-wrap">
            <ul class="header-nav">
                <li class="current"><a href="https://minglunwu.github.io/#home" title="home">Home</a></li>
                <li><a href="https://minglunwu.github.io/#about" title="about">About</a></li>
                <li><a href="https://minglunwu.github.io/#blog" title="blog">Blogs</a></li>
                <li><a href="https://minglunwu.github.io/#note" title="note">Notes</a></li>
                <!--<li><a class="smoothscroll" href="#note" title="notes">Notes</a></li>-->
                <!--<li><a class="smoothscroll"  href="#contact" title="contact">Contact</a></li>-->
            </ul>
        </nav>

        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->

        <article class="blog-single">

            <!-- page header/blog hero
            ================================================== -->
            <div class="page-header page-header--single page-hero" style="background-image:url(https://images.unsplash.com/photo-1554631995-6c22c6e729fe?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=733&q=80)">
            
                <div class="row page-header__content narrow">
                    <article class="col-full">
                        <div class="page-header__info">
                            <div class="page-header__cat">
                                    <a href="#0">transformers</a>
                            </div>
                        </div>
                        <h1 class="page-header__title">
                            <a href="#0" title="">
                                NLPæ·±åº¦å­¸ç¿’ä¸èƒ½ä¸ç”¨çš„å¥—ä»¶ - Transformers
                            </a>
                        </h1>
                        <ul class="page-header__meta">
                            <li class="date">2020- 6-18 (å››)</li>
                            <li class="author">
                                By
                                <span>MingLun Allen Wu</span>
                            </li>
                        </ul>
                        
                    </article>
                </div>
        
            </div> <!-- end page-header -->
    
            <div class="row blog-content">
                <div class="col-full blog-content__main">
                    
                    <ul>
<li><a href="#å‰è¨€">å‰è¨€</a></li>
<li><a href="#å¦‚ä½•ä½¿ç”¨transformers">å¦‚ä½•ä½¿ç”¨Transformers</a></li>
<li><a href="#tokenizer">Tokenizer</a><ul>
<li><a href="#1-åˆå§‹åŒ–">1. åˆå§‹åŒ–</a></li>
<li><a href="#2-åŸºæœ¬ä½¿ç”¨">2. åŸºæœ¬ä½¿ç”¨</a></li>
</ul>
</li>
<li><a href="#pre-trained-language-model">Pre-trained Language Model</a><ul>
<li><a href="#1-åˆå§‹åŒ–-1">1. åˆå§‹åŒ–</a></li>
<li><a href="#2-åŸºæœ¬ä½¿ç”¨-1">2. åŸºæœ¬ä½¿ç”¨</a></li>
</ul>
</li>
<li><a href="#transformers-çš„å„ªå‹¢">Transformers çš„å„ªå‹¢</a></li>
<li><a href="#å¾Œè¨˜">å¾Œè¨˜</a></li>
</ul>
<p id="å‰è¨€"></p>

<h1>å‰è¨€</h1>
<p>æœ¬ç¯‡ç­†è¨˜æ‰€ç´€éŒ„çš„ Transformers ä¸¦ä¸æ˜¯ Attention è«–æ–‡ä¸­æ‰€æåˆ°çš„ "Transformer"æ¨¡å‹ï¼Œè€Œæ˜¯ç”±Higginfaceåœ˜éšŠæ‰€é–‹ç™¼çš„ <code>Transformers</code> å¥—ä»¶ã€‚</p>
<p>é€™å‰‡ç­†è¨˜çš„é‡é»åœ¨æ–¼:  </p>
<ol>
<li>ç‚ºä»€éº¼è¦ä½¿ç”¨ Transformers</li>
<li>Transformers çš„å…©å¤§å…ƒä»¶:<ul>
<li>Tokenizer</li>
<li>Model</li>
</ul>
</li>
<li>ä½¿ç”¨ Transformers å¯¦ä½œ Feature Extraction.</li>
</ol>
<p>è¿‘å¹´ä¾† NLP åœ¨ Attention æ¦‚å¿µæå‡ºä»¥å¾Œï¼Œå„ç¨®æ¨¡å‹å¦‚åŒé›¨å¾Œæ˜¥ç­èˆ¬å™´ç™¼ï¼š
<img style="display:block; margin-left:auto; margin-right:auto; width:100%;" src="https://miro.medium.com/max/1400/1*corMthPJwan-yw0KOcZ6qQ.png">
(åœ–ç‰‡ä¾†æº: <a href="https://medium.com/@hamdan.hussam/from-bert-to-albert-pre-trained-langaug-models-5865aa5c3762">https://medium.com/@hamdan.hussam/from-bert-to-albert-pre-trained-langaug-models-5865aa5c3762</a>)</p>
<p>ç„¶è€Œåœ¨ä½¿ç”¨ä¸åŒçš„æ¨¡å‹æ™‚ï¼Œæ¯å€‹æ¨¡å‹çš„æ¶æ§‹ã€åƒæ•¸çš„è¼‰å…¥æ–¹å¼éƒ½ä¸ç›¸åŒï¼Œå¤§å¤§çš„æé«˜äº†æ¨¡å‹é–“çš„è½‰æ›æˆæœ¬ã€‚èˆ‰ä¾‹ä¾†èªªï¼š é€²è¡Œå‡æ–°èåˆ†é¡ä»»å‹™æ™‚ï¼Œå¯èƒ½æœƒæƒ³è¦å˜—è©¦å¾ã€ŒBERTã€è½‰æ›åˆ°ã€ŒXLNetã€ï¼Œåœ¨å…¶é¤˜æ¶æ§‹ä¸è®Šçš„å‰æä¸‹ï¼Œå…‰æ˜¯é€²è¡Œ Pre-train Language Model çš„æ›´æ›å¯èƒ½å°±æœƒèŠ±è²»ä¸å°‘æ™‚é–“ã€‚</p>
<p>é€é <code>Transformers</code> é€™å€‹å¥—ä»¶ï¼Œèƒ½å¤ å°‡è½‰æ›æˆæœ¬é™åˆ°æœ€ä½ï¼ ä½¿ç”¨é€™å¥—æ¡†æ¶èƒ½å¤ ç›´æ¥å°å¤šç¨®æ¨¡å‹é€²è¡Œæ“ä½œï¼Œå„ç¨®æ¨¡å‹çš„æ¶æ§‹åŠåƒæ•¸éƒ½å·²ç¶“è¢«å°è£åœ¨å¥—ä»¶ä¸­ï¼Œåªéœ€è¦äº†è§£æ­¤æ¡†æ¶çš„æ©Ÿåˆ¶ï¼Œå³å¯å¿«é€Ÿå¥—ç”¨åˆ°æ•¸åç¨®ä¸åŒçš„æ¨¡å‹ï¼</p>
<blockquote>
<p>ç›´ç™½çš„èªªï¼šå­¸ç¿’é€™å€‹å¥—ä»¶ï¼Œå°±èƒ½åŒæ™‚å­¸æœƒä½¿ç”¨å¤šç¨®ä¸»æµçš„èªè¨€æ¨¡å‹ã€‚</p>
</blockquote>
<p>é™„ä¸Š <code>Transfomers</code> çš„ å®˜æ–¹ç¶²ç«™:</p>
<p><a href="https://huggingface.co/transformers/">Hugging Face - Transformers</a></p>
<p>ä»¥åŠ <code>Transformers</code> ç•¶å‰æ‰€æ”¯æ´çš„æ¨¡å‹æ¸…å–®:</p>
<p><a href="https://huggingface.co/models">Hugging Face - On a mission to solve NLP, one commit at a time.</a></p>
<hr>
<p><p id="å¦‚ä½•ä½¿ç”¨transformers"></p></p>
<h1>å¦‚ä½•ä½¿ç”¨Transformers</h1>
<p>æˆ‘å°‡ <code>Transformers</code> å¥—ä»¶åˆ†æˆå…©å€‹éƒ¨åˆ†: </p>
<ol>
<li><strong>Tokenizer</strong>  : å°‡æ–‡å­—æ–·è©å¾Œè½‰æ›ç‚º Index. </li>
<li><strong>Pre-trained Model</strong> :  æ¥å—è½‰æ›çš„ Indexï¼Œ è¼¸å‡º Word Representation.</li>
</ol>
<p>ä¸‹é¢é€™å¼µæµç¨‹åœ–ï¼Œæ˜¯å¾æ–‡å­—è½‰æ›æˆ Word Representationçš„éç¨‹ï¼Œæˆ‘å€‘è—‰æ­¤ä¾†äº†è§£é€™å…©å€‹å…ƒä»¶çš„ä½œç”¨(åé»ƒ)åˆ°åº•æ˜¯ä»€éº¼ :
<img style="display:block; margin-left:auto; margin-right:auto; width:100%;" src="https://minglunwu.github.io/images/20200618/word2vec_flow-2.png"></p>
<p id="tokenizer"></p>

<h2>Tokenizer</h2>
<p id="1-åˆå§‹åŒ–"></p>

<h3>1. åˆå§‹åŒ–</h3>
<p>Tokenizer çš„åŠŸç”¨æ˜¯å°‡æ–‡å­—é€²è¡Œæ–·è©åŠè½‰æ›ç‚ºIndexï¼Œä¸åŒçš„ Pre-trained Language Model æ‰€ä½¿ç”¨çš„æ–·è©æ–¹å¼ä»¥åŠ Index ä¹Ÿä¸ç›¡ç›¸åŒï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨å‰éœ€è¦å…ˆé€²è¡Œã€Œåˆå§‹åŒ–ã€ã€‚</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BertTokenizer</span>

<span class="c1"># å¸¸è¦‹æ¨¡å‹å…·æœ‰è‡ªå·±çš„Tokenizer</span>
<span class="n">tokenizer_bert</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>

<span class="c1"># AutoTokenizer å‰‡æ˜¯é€šç”¨å‹</span>
<span class="n">tokenizer_other</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;emilyalsentzer/Bio_ClinicalBERT&quot;</span><span class="p">)</span>
</pre></div>


<p>é‡å°è¼ƒå¸¸è¦‹çš„èªè¨€æ¨¡å‹ (BERT, GPT2, XLM, XLNet...)ï¼Œ <code>Transformers</code>æœ‰æä¾›å°ˆå±¬çš„ Tokenizerå¯ä»¥ä½¿ç”¨ã€‚ </p>
<p>é™¤æ­¤ä¹‹å¤–ï¼Œé‚„æœ‰é¡å¤–å®šç¾©ä¸€å€‹ <code>AutoTokenizer</code>ï¼Œå¯ä»¥è®“ä½¿ç”¨è€…äº’ç›¸åˆ†äº«ã€ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ã€‚  é€é <code>tokenizer.from_pretrained(&lt;model_name&gt;)</code>å³å¯ä¸‹è¼‰ä¸¦ä¸”è¼‰å…¥è©²æ¨¡å‹çš„ç›¸é—œè¨­å®šã€‚</p>
<p>åœ¨åˆæ¬¡ä½¿ç”¨æ¨¡å‹æ™‚ï¼Œ <code>transformers</code>æœƒè‡ªå‹•ä¸‹è¼‰è©²æ¨¡å‹çš„æ¬Šé‡åŠç›¸é—œè¨­å®šï¼Œéœ€è¦èŠ±è²»ä¸€é»æ™‚é–“ (æ™‚é–“é•·åº¦ç«¯çœ‹æ¨¡å‹çš„å¤§å°)ï¼Œä¸‹è¼‰å®Œæˆå¾Œï¼Œå¾€å¾Œçš„åŸ·è¡Œå°‡æœƒå¾æœ¬åœ°ç«¯çš„å¿«å–è³‡æ–™å¤¾è¼‰å…¥ï¼Œå°±ä¸éœ€è¦å†é€²è¡Œä¸‹è¼‰äº†ã€‚</p>
<p id="2-åŸºæœ¬ä½¿ç”¨"></p>

<h3>2. åŸºæœ¬ä½¿ç”¨</h3>
<div class="highlight"><pre><span></span><span class="n">tokenizer_bert</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>
<span class="n">test_input</span> <span class="o">=</span> <span class="s2">&quot;I don&#39;t want to work.&quot;</span>

<span class="n">encode_result</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>  <span class="c1"># ä½¿ç”¨ encode() ä¾†å°æ–‡å­—é€²è¡Œæ–·è©ã€ç·¨ç¢¼ã€‚</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span>
<span class="c1"># [101, 146, 1274, 112, 189, 1328, 1106, 1250, 119, 102]</span>

<span class="n">decode_result</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span> <span class="c1"># ä½¿ç”¨ decode() ä¾†å°‡ id è½‰æ›å›æ–‡å­—ã€‚</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decode_result</span><span class="p">)</span>
<span class="c1"># &quot;[CLS] I don&#39;t want to work. [SEP]&quot;</span>
</pre></div>


<p>å¾ä¸­æˆ‘å€‘è§€å¯Ÿåˆ°å…©ä»¶äº‹æƒ…ï¼š</p>
<ol>
<li>
<p><code>encode_result</code> çš„é•·åº¦å¥½åƒè·Ÿ <code>test_input</code> çš„é•·åº¦ä¸ç›¸åŒï¼: </p>
<p>é€™æ˜¯å› ç‚ºæœ‰äº›æ¨¡å‹æœƒä½¿ç”¨ Word Piece Tokenizerï¼Œä¹Ÿå°±æ˜¯åœ¨æ–·è©éç¨‹ä¸­å°‡è©å½™é€²è¡Œæ‹†è§£ï¼Œæ‰€ä»¥æœƒå°è‡´Encodeéçš„çµæœé•·åº¦èˆ‡åŸå§‹Inputä¸åŒï¼</p>
</li>
<li>
<p>Decodeå¾Œçš„çµæœå¥½åƒè·ŸåŸæœ¬çš„ä¸å¤ªä¸€æ¨£...: </p>
<p>å› ç‚ºåœ¨ <code>tokenizer.encode()</code>éç¨‹ä¸­æœƒè‡ªå‹•åŠ å…¥ [CLS]ã€[SEP]ç­‰ç‰¹æ®Šæ¨™è¨˜ï¼Œé€™æ˜¯ç”±æ–¼ BERT æ¨¡å‹çš„æ©Ÿåˆ¶è€Œè‡ªå‹•ç”¢ç”Ÿï¼Œå…¶é¤˜æ¨¡å‹çš„ Tokenizer ä¸ä¸€å®šæœƒæœ‰ï¼Œä¹Ÿå¯ä»¥åœ¨encodeéç¨‹ä¸­åŠ å…¥åƒæ•¸ä¾†ç§»é™¤:</p>
<div class="highlight"><pre><span></span><span class="n">encode_result</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># ç·¨ç¢¼éç¨‹ä¸­ï¼Œç§»é™¤ç‰¹æ®Šç¬¦è™Ÿ</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span> 
<span class="c1"># [146, 1274, 112, 189, 1328, 1106, 1250, 119]</span>

<span class="n">decode_result</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span> <span class="c1"># ä½¿ç”¨ decode() ä¾†å°‡ id è½‰æ›å›æ–‡å­—ã€‚</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decode_result</span><span class="p">)</span>
<span class="c1"># &quot;I don&#39;t want to work.&quot;</span>
</pre></div>


</li>
</ol>
<p>é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨é€²è¡Œè³‡æ–™å‰è™•ç†æ™‚ï¼Œå¦å¤–ä¸€å€‹éº»ç…©çš„å•é¡Œæ˜¯åœ¨ç”¢ç”Ÿ "batch"æ™‚ï¼Œå¿…é ˆè¦è®“æ¯ä¸€ç­†è³‡æ–™çš„é•·åº¦ç›¸åŒ(ä¹Ÿå°±æ˜¯è¦é€²è¡Œ"Padding")ï¼Œå‡è¨­æœ€å¤§é•·åº¦è¨­å®šç‚º <code>64</code>: </p>
<ul>
<li>é•·åº¦ä¸è¶³çš„å¥å­éœ€è¦è£œé½Š "[PAD]"ç¬¦è™Ÿï¼Œä½¿é•·åº¦é”åˆ° 64ã€‚</li>
<li>é•·åº¦éé•·çš„å¥å­éœ€è¦åªä¿ç•™å‰64å€‹å­—ï¼Œè¶…éçš„éƒ¨åˆ†æ¨æ£„ä¸ç”¨ã€‚</li>
</ul>
<p>é€™ä»¶äº‹æƒ…å¦‚æœè‡ªå·±è™•ç†ï¼Œæœƒæœ‰é»å°éº»ç…©ï¼Œå¹¸å¥½ Tokenizer éƒ½å¹«å¿™åšå¥½äº†.</p>
<div class="highlight"><pre><span></span><span class="n">test_input</span> <span class="o">=</span> <span class="s2">&quot;I don&#39;t want to work.&quot;</span>

<span class="n">encode_result</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">pad_to_max_length</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span>
<span class="c1"># [101, 146, 1274, 112, 189, 1328, 1106, 1250, 119, 102, 0, 0, 0,.... 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span>

<span class="n">decode_result</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decode_result</span><span class="p">)</span>
<span class="c1"># &quot;[CLS] I don&#39;t want to work. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]...[PAD]&quot;</span>
</pre></div>


<p>é€é Tokenizer é€²è¡Œæ–·è©åŠç·¨ç¢¼å¾Œï¼Œæˆ‘å€‘å°±èƒ½å°‡å…¶é€å…¥ Pre-trained Model ä¸­å–å¾— Representationäº†ï¼</p>
<hr>
<p id="pre-trained-language-model"></p>

<h2>Pre-trained Language Model</h2>
<p id="1-åˆå§‹åŒ–-1"></p>

<h3>1. åˆå§‹åŒ–</h3>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">BertModel</span>

<span class="n">model_bert</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>
<span class="n">model_other</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;emilyalsentzer/Bio_ClinicalBERT&quot;</span><span class="p">)</span>
</pre></div>


<p>èˆ‡ <code>Tokenizer</code>çš„æ©Ÿåˆ¶ç›¸åŒï¼Œ <code>Transformers</code>ä¸­ä¹ŸåŒ…å«äº†å„ç¨®é å…ˆå®šç¾©å¥½çš„ Model (ä¾‹å¦‚ <code>BertModel</code>)ï¼Œä»¥åŠæ–¹ä¾¿ä½¿ç”¨å…¶ä»–é–‹æºæ¨¡å‹çš„æ³›ç”¨Model (<code>AutoModel</code>)ã€‚</p>
<p>åœ¨ä½¿ç”¨å‰ä¹Ÿéœ€è¦é€é <code>model.from_pretrained(&lt;model_name&gt;)</code>ä¾†æŒ‡å®šè¦è¼‰å…¥çš„æ¨¡å‹ã€‚</p>
<p>èˆ‡ <code>Tokenizer</code>ç›¸åŒï¼Œåˆæ¬¡ä½¿ç”¨æ¨¡å‹æ™‚æœƒéœ€è¦é€²è¡Œä¸‹è¼‰ï¼Œæ‰€ä»¥éœ€è¦ç­‰å¾…ä¸€æ®µæ™‚é–“ï¼Œä»¥<code>xlnet-large-cased</code>ç‚ºä¾‹ï¼Œå¤§ç´„æœ‰3.3Géœ€è¦é€²è¡Œä¸‹è¼‰ã€‚</p>
<p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼š<code>Tokenizer</code> åŠ <code>Model</code> æ˜¯æœ‰å…ˆå¾Œé—œä¿‚çš„ï¼š
1. æ–‡å­—é€²å…¥<code>Tokenizer</code>è½‰æ›ã€‚
2. è½‰æ›å¾Œçš„Indexé€å…¥<code>Model</code>ä¸­å–å¾—Word Representationã€‚</p>
<p>æ‰€ä»¥ <code>Tokenizer</code> åŠ <code>Model</code> çš„ <code>&lt;model_name&gt;</code>å¿…éœ€è¦ä¸€è‡´ï¼Œä»¥å…ç™¼ç”ŸéŒ¯èª¤ã€‚</p>
<p id="2-åŸºæœ¬ä½¿ç”¨-1"></p>

<h3>2. åŸºæœ¬ä½¿ç”¨</h3>
<div class="highlight"><pre><span></span><span class="n">tokenizer_bert</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>
<span class="n">test_input</span> <span class="o">=</span> <span class="s2">&quot;I don&#39;t want to work.&quot;</span>

<span class="n">encode_result</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">pad_to_max_length</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># ä½¿ç”¨ encode() ä¾†å°æ–‡å­—é€²è¡Œæ–·è©ã€ç·¨ç¢¼ã€‚</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span>
<span class="c1"># [101, 146, 1274, 112, 189, 1328, 1106, 1250, 119, 102, 0, 0, ..., 0]</span>

<span class="n">model_bert</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model_bert</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span> <span class="c1"># ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.</span>
<span class="n">last_hidden_layer</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># é€™å°±æ˜¯ â€œtest_input&quot; çš„ word representation.</span>

<span class="nb">print</span><span class="p">(</span><span class="n">last_hidden_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># (1, 64, 768)   =&gt; (batch, words, word vector dimension)</span>
</pre></div>


<p><code>transformers</code>ä¸­çš„æ‰€æœ‰ <code>model</code> éƒ½æ˜¯ç¹¼æ‰¿ <code>torch.nn.Module</code>è€Œä¾†ï¼Œåœ¨ä½¿ç”¨ä¸Šå…¶å¯¦è·Ÿ Pytorch çš„ Model ç›¸ç•¶é¡ä¼¼ï¼ æ‰€ä»¥åœ¨ä¸Šè¿°çš„èªæ³•ä¸­å°‡ <code>encode_result</code> é€å…¥ <code>model_bert</code>ä¸­ï¼Œå°±ç­‰åŒæ–¼å°‡ <code>encode_result</code>é€å…¥ <code>model</code> ä¸­é€²è¡Œ Forward Propagationï¼Œæœ€å¾Œå³å¯å¾—åˆ° Word Representation.</p>
<p>è‡³æ–¼ç‚ºä»€éº¼ <code>last_hidden_layer = output[0]</code>ï¼Œå¯ä»¥åƒè€ƒ BertModel çš„ Return æ¬„ä½ï¼Œå›å‚³çš„çµæœæœ‰è¨±å¤šå…§å®¹ï¼Œè€Œç¬¬ä¸€å€‹å°±æ˜¯ã€Œæœ€å¾Œä¸€å±¤çš„ Hidden Statesã€ä¹Ÿå°±æ˜¯æˆ‘å€‘è¦çš„ Word Representation.</p>
<p>é™„ä¸Š <code>BertModel</code>çš„å®˜æ–¹æ–‡ä»¶é€£çµï¼š
<a href="https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel.forward">BERT - transformers 2.11.0 documentation</a></p>
<hr>
<p id="transformers-çš„å„ªå‹¢"></p>

<h1>Transformers çš„å„ªå‹¢</h1>
<p>äº†è§£ <code>Transformers</code>çš„åŸºæœ¬é‹ä½œåŸç†å¾Œï¼Œå†å›é ­å¼·èª¿ä¸€æ¬¡å®ƒçš„å„ªå‹¢ï¼: </p>
<ol>
<li>
<p><strong>å¯è¦–ç‚ºå°è£å¥½çš„ Language Model :</strong></p>
<p>Transformers çš„ Model ç¹¼æ‰¿äº† <code>torch.nn.Module</code>ï¼Œä¹Ÿå°±æ˜¯èªªåœ¨è¨­è¨ˆæ•´å€‹ä»»å‹™çš„éç¨‹ä¸­ï¼Œä¸éœ€è¦è‡ªå·±å»å®šç¾© "Pre-trained Language Model" çš„çµæ§‹ã€ä¹Ÿä¸éœ€è¦è‡ªè¡Œè¼‰å…¥æ¬Šé‡ï¼Œ<strong>åªéœ€è¦å°‡å…¶è¦–ç‚ºä¸€å€‹Pytorch é å…ˆå®šç¾©å¥½çš„ Model ï¼š</strong>å¯¦ä½œä¸€å€‹ <code>Transformers</code>çš„Modelã€è¼‰å…¥ç›¸é—œæ¬Šé‡ï¼Œæ¥ä¸‹ä¾†åªéœ€è¦çµ„è£å°±å¥½äº†ï¼</p>
<div class="highlight"><pre><span></span><span class="n">model_bert</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>
<span class="n">model_ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model_bert</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span> <span class="c1"># ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.</span>
<span class="n">last_hidden_layer</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model_ln1</span><span class="p">(</span><span class="n">last_hidden_layer</span><span class="p">)</span> <span class="c1"># å¯ä»¥ç›´æ¥å†å°‡ BertModel çš„ Outputé€å…¥å…¶ä»–è‡ªå®šç¾©çš„çµæ§‹ã€‚</span>
</pre></div>


<p>äº†è§£åˆ°å¦‚ä½•çµ„è£ã€å¦‚ä½•ä½¿ç”¨å¾Œï¼Œé™¤äº†å°‡å…¶æ‹¿ä¾†åš Feature Extractionå¤–ï¼Œä¹Ÿå¯ä»¥ç”¨ç›¸åŒçš„æ¦‚å¿µçµ„è£æˆ Transfer Learning çš„æ¨¡å‹æ¶æ§‹ï¼ä¸éé€™è¶…å‡ºé€™å‰‡ç­†è¨˜æƒ³è¦ç´€éŒ„çš„ç¯„åœäº†ï½</p>
<p>å¦å¤–ï¼Œç¿’æ…£ä½¿ç”¨ Tensorflow çš„æœ‹å‹ä¹Ÿåˆ¥å‚·å¿ƒï¼Œ <code>transformers</code>ä¹Ÿæœ‰æä¾› tensorflowç‰ˆæœ¬çš„ Model (ä¾‹å¦‚ <code>tfBertModel</code>)!</p>
</li>
<li>
<p><strong>åŒæ¨£çš„æ¶æ§‹èƒ½ä½¿ç”¨å¤šç¨®æ¨¡å‹ â†’ æ–¹ä¾¿æŠ½æ›</strong></p>
<p>ç”±æ–¼ <code>Transformers</code> ä¸­åŒ…å«è¨±å¤šæ¨¡å‹ï¼šæœ‰éƒ¨åˆ†æ˜¯å®˜æ–¹æŒçºŒæ–°å¢ï¼Œæœ‰äº›å‰‡æ˜¯ä½¿ç”¨è€…äº’ç›¸é‡‹å‡ºï¼Œå°è‡´ä½¿ç”¨é€™å¥—æ¶æ§‹å°±èƒ½å¿«é€Ÿåˆ‡æ›å¤šç¨®æ¨¡å‹ã€‚</p>
<div class="highlight"><pre><span></span><span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;bert-base-cased&quot;</span>
<span class="n">model_language</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL</span><span class="p">)</span>
<span class="n">model_ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model_language</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span> <span class="c1"># ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.</span>
<span class="n">last_hidden_layer</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model_ln1</span><span class="p">(</span><span class="n">last_hidden_layer</span><span class="p">)</span> <span class="c1"># å¯ä»¥ç›´æ¥å†å°‡ model_language çš„ Outputé€å…¥å…¶ä»–è‡ªå®šç¾©çš„çµæ§‹ã€‚</span>
</pre></div>


<p>ç›¸åŒçš„ç¨‹å¼ç¢¼ï¼Œåªéœ€è¦æ›´æ›æ¨¡å‹è®Šæ•¸å³å¯é”åˆ°ã€ŒæŠ½æ›åº•å±¤ Pre-trained Langauge Modelçš„æ•ˆæœã€</p>
<div class="highlight"><pre><span></span><span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;xlnet-large-cased&quot;</span>
<span class="n">model_language</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL</span><span class="p">)</span>
<span class="n">model_ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model_language</span><span class="p">(</span><span class="n">encode_result</span><span class="p">)</span> <span class="c1"># ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.</span>
<span class="n">last_hidden_layer</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model_ln1</span><span class="p">(</span><span class="n">last_hidden_layer</span><span class="p">)</span> <span class="c1"># å¯ä»¥ç›´æ¥å†å°‡ model_language çš„ Outputé€å…¥å…¶ä»–è‡ªå®šç¾©çš„çµæ§‹ã€‚</span>
</pre></div>


</li>
</ol>
<hr>
<p id="å¾Œè¨˜"></p>

<h1>å¾Œè¨˜</h1>
<p>äº†è§£ <code>Transformers</code> é€™å€‹å¥—ä»¶å¾Œï¼Œåœ¨é€²è¡Œ NLP çš„ç›¸é—œä»»å‹™ä¸Šç¯€çœäº†è¨±å¤šæ™‚é–“ï¼Œå¯ä»¥å¿«é€Ÿåœ°åˆ‡æ›Pre-trained Language Model ä¾†é€²è¡Œå„ç¨®å˜—è©¦ã€‚</p>
<p>æ­¤å¤–ï¼Œåœ¨Surveyæ–°çš„Language Modelæ™‚ï¼Œå¦‚æœåœ¨Githubä¸Šé¢çœ‹åˆ°å®ƒæ”¯æ´ <code>transformers</code>ï¼Œæœƒå‚™æ„Ÿæ¬£æ…°QQï¼Œé “æ™‚æ„Ÿåˆ°ä¸–ç•Œçš„ç¾å¥½ï½</p>
<p>æ„Ÿè¬ä½ çš„é–±è®€ï¼Œå¸Œæœ›ä½ ä¹Ÿèƒ½è·Ÿæˆ‘ä¸€èµ·æ„Ÿå—ä¸–ç•Œçš„ç¾å¥½ï¼XD </p>
<p>æœ‰ä»»ä½•å•é¡Œæ­¡è¿ä¸€èµ·äº¤æµï¼ ä¸‹æ¬¡è¦‹ï¼</p>
    
                    <p class="blog-content__tags">
                        <span>Post Tags</span>
    
                        <span class="blog-content__tag-list">
                             <a href="#0">transformers</a>
                        </span>
    
                    </p>
    
                    <div class="blog-content__pagenav">
                        <div class="blog-content__nav">




















                            <div class="blog-content__prev">
                                <a href="https://minglunwu.github.io/notes/2020/gitlab-ci-cd.html" rel="prev">
                                    <span>Previous Post</span>
                                    é€²è·å ´å­¸åˆ°çš„Git - (1) Gitlab CI/CD
                                </a>
                            </div>
                            <div class="blog-content__next">
                                <a href="https://minglunwu.github.io/notes/2020/20200527.html" rel="next">
                                    <span>Next Post</span>
                                    ä¸å†è¢«å¤§é‡è¶…åƒæ•¸åŠæ¨¡å‹è¡¨ç¾æ·¹æ²’ï¼ä½¿ç”¨MLFlowé€²è¡Œå¯¦é©—ç®¡ç†
                                </a>
                            </div>
                        </div>
    
                        <div class="blog-content__all">
                            <a href="https://minglunwu.github.io/archives.html" class="btn btn--primary">
                                View All Post
                            </a>
                        </div>
                    </div>
    
                </div><!-- end blog-content__main -->
            </div> <!-- end blog-content -->
    
        </article>
            <!-- footer
    ================================================== -->
    <footer>
        <div class="row">
            <div class="col-full">

                <div class="footer-logo">
                    <a class="footer-site-logo" href="#0"><img src="https://minglunwu.github.io/theme/images/logo.png" alt="Homepage"></a>
                </div>
                <ul class="footer-social">
                    <li>
                        <a href="https://www.facebook.com/zebra52000" target="_blank"><i class="im im-facebook" aria-hidden="true"></i><span>Facebook</span></a>
                    </li>
                    <li>
                        <a href="https://medium.com/@allen6997535" target="_blank"><i class="im im-book" aria-hidden="true"></i><span>Medium</span></a>
                    </li>
                    <li>
                        <a href="https://github.com/MingLunWu" target="_blank"><i class="im im-github" aria-hidden="true"></i><span>Github</span></a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/allen-ming-lun-wu-637020142/" target="_blank"><i class="im im-linkedin" aria-hidden="true"></i><span>LinkedIn</span></a>
                    </li>
                    <li>
                        <a href="https://www.instagram.com/whoisfater0724/" target="_blank"><i class="im im-instagram" aria-hidden="true"></i><span>Instagram</span></a>
                    </li>
                </ul>
                    
            </div>
        </div>

        <div class="row footer-bottom">

            <div class="col-twelve">
                <div class="copyright">
                    <span>Â© Copyright Hola 2017</span> 
                    <span>Design by <a href="https://www.styleshout.com/">styleshout</a></span>	
                </div>

                <div class="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
                </div>
            </div>

        </div> <!-- end footer-bottom -->

    </footer> <!-- end footer -->
    <!-- Java Script
        ================================================== -->
        <script src="https://minglunwu.github.io/theme/js/jquery-3.2.1.min.js"></script>
        <script src="https://minglunwu.github.io/theme/js/plugins.js"></script>
        <script src="https://minglunwu.github.io/theme/js/main.js"></script>

    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-161863471-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-161863471-1');
    </script>
    
</body>