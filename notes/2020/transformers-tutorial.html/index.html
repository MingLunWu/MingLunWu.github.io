<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="MingLun Blog"><meta property="og:type" content="article"><meta property="og:image" content="https://images.unsplash.com/photo-1554631995-6c22c6e729fe?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=733&q=80"><meta property="twitter:image" content="https://images.unsplash.com/photo-1554631995-6c22c6e729fe?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=733&q=80"><meta name=title content="NLPæ·±åº¦å­¸ç¿’ä¸èƒ½ä¸ç”¨çš„å¥—ä»¶ - Transformers"><meta property="og:title" content="NLPæ·±åº¦å­¸ç¿’ä¸èƒ½ä¸ç”¨çš„å¥—ä»¶ - Transformers"><meta property="twitter:title" content="NLPæ·±åº¦å­¸ç¿’ä¸èƒ½ä¸ç”¨çš„å¥—ä»¶ - Transformers"><meta name=description content><meta property="og:description" content><meta property="twitter:description" content><meta property="twitter:card" content="é€éTransformerså¥—ä»¶ä½¿ç”¨å¤šç¨®Pre-trained Language Modelï¼Œä¸¦ä¸”å¯¦ä½œText Feature Extraction."><meta name=keyword content="å³æ˜å€«, MingLun, minglunwu"><link rel="shortcut icon" href=/img/favicon.ico><title>NLPæ·±åº¦å­¸ç¿’ä¸èƒ½ä¸ç”¨çš„å¥—ä»¶ - Transformers | MingLun Blog</title><link rel=canonical href=/notes/2020/transformers-tutorial.html/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script>
<script src=/js/bootstrap.min.js></script>
<script src=/js/hux-blog.min.js></script>
<script src=/js/lazysizes.min.js></script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-NC508K3RBY"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NC508K3RBY",{anonymize_ip:!1})}</script><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button>
<a class=navbar-brand href=/>MingLun Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.unsplash.com/photo-1554631995-6c22c6e729fe?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=733&q=80)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/transformers title=transformers>transformers</a></div><h1>NLPæ·±åº¦å­¸ç¿’ä¸èƒ½ä¸ç”¨çš„å¥—ä»¶ - Transformers</h1><h2 class=subheading></h2><span class=meta>Posted by
MingLun Allen Wu
on
Thursday, June 18, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><ul><li><a href=#%E5%89%8D%E8%A8%80>å‰è¨€</a></li><li><a href=#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8transformers>å¦‚ä½•ä½¿ç”¨Transformers</a><ul><li><a href=#tokenizer>Tokenizer</a><ul><li><a href=#1-%E5%88%9D%E5%A7%8B%E5%8C%96>1. åˆå§‹åŒ–</a></li><li><a href=#2-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8>2. åŸºæœ¬ä½¿ç”¨</a></li></ul></li><li><a href=#pre-trained-language-model>Pre-trained Language Model</a><ul><li><a href=#1-%E5%88%9D%E5%A7%8B%E5%8C%96-1>1. åˆå§‹åŒ–</a></li><li><a href=#2-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8-1>2. åŸºæœ¬ä½¿ç”¨</a></li></ul></li></ul></li><li><a href=#transformers-%E7%9A%84%E5%84%AA%E5%8B%A2>Transformers çš„å„ªå‹¢</a></li><li><a href=#%E5%BE%8C%E8%A8%98>å¾Œè¨˜</a></li></ul><p>æœ¬ç¯‡ç­†è¨˜æ‰€ç´€éŒ„çš„ Transformers ä¸¦ä¸æ˜¯ Attention è«–æ–‡ä¸­æ‰€æåˆ°çš„ &ldquo;Transformer"æ¨¡å‹ï¼Œè€Œæ˜¯ç”±Higginfaceåœ˜éšŠæ‰€é–‹ç™¼çš„ <code>Transformers</code> å¥—ä»¶ã€‚</p><p>é€™å‰‡ç­†è¨˜çš„é‡é»åœ¨æ–¼:</p><ol><li>ç‚ºä»€éº¼è¦ä½¿ç”¨ Transformers</li><li>Transformers çš„å…©å¤§å…ƒä»¶:<ul><li>Tokenizer</li><li>Model</li></ul></li><li>ä½¿ç”¨ Transformers å¯¦ä½œ Feature Extraction.</li></ol><p>è¿‘å¹´ä¾† NLP åœ¨ Attention æ¦‚å¿µæå‡ºä»¥å¾Œï¼Œå„ç¨®æ¨¡å‹å¦‚åŒé›¨å¾Œæ˜¥ç­èˆ¬å™´ç™¼ï¼š
(åœ–ç‰‡ä¾†æº: <a href=https://medium.com/@hamdan.hussam/from-bert-to-albert-pre-trained-langaug-models-5865aa5c3762>https://medium.com/@hamdan.hussam/from-bert-to-albert-pre-trained-langaug-models-5865aa5c3762</a>)</p><p>ç„¶è€Œåœ¨ä½¿ç”¨ä¸åŒçš„æ¨¡å‹æ™‚ï¼Œæ¯å€‹æ¨¡å‹çš„æ¶æ§‹ã€åƒæ•¸çš„è¼‰å…¥æ–¹å¼éƒ½ä¸ç›¸åŒï¼Œå¤§å¤§çš„æé«˜äº†æ¨¡å‹é–“çš„è½‰æ›æˆæœ¬ã€‚èˆ‰ä¾‹ä¾†èªªï¼š é€²è¡Œå‡æ–°èåˆ†é¡ä»»å‹™æ™‚ï¼Œå¯èƒ½æœƒæƒ³è¦å˜—è©¦å¾ã€ŒBERTã€è½‰æ›åˆ°ã€ŒXLNetã€ï¼Œåœ¨å…¶é¤˜æ¶æ§‹ä¸è®Šçš„å‰æä¸‹ï¼Œå…‰æ˜¯é€²è¡Œ Pre-train Language Model çš„æ›´æ›å¯èƒ½å°±æœƒèŠ±è²»ä¸å°‘æ™‚é–“ã€‚</p><p>é€é <code>Transformers</code> é€™å€‹å¥—ä»¶ï¼Œèƒ½å¤ å°‡è½‰æ›æˆæœ¬é™åˆ°æœ€ä½ï¼ ä½¿ç”¨é€™å¥—æ¡†æ¶èƒ½å¤ ç›´æ¥å°å¤šç¨®æ¨¡å‹é€²è¡Œæ“ä½œï¼Œå„ç¨®æ¨¡å‹çš„æ¶æ§‹åŠåƒæ•¸éƒ½å·²ç¶“è¢«å°è£åœ¨å¥—ä»¶ä¸­ï¼Œåªéœ€è¦äº†è§£æ­¤æ¡†æ¶çš„æ©Ÿåˆ¶ï¼Œå³å¯å¿«é€Ÿå¥—ç”¨åˆ°æ•¸åç¨®ä¸åŒçš„æ¨¡å‹ï¼</p><blockquote><p>ç›´ç™½çš„èªªï¼šå­¸ç¿’é€™å€‹å¥—ä»¶ï¼Œå°±èƒ½åŒæ™‚å­¸æœƒä½¿ç”¨å¤šç¨®ä¸»æµçš„èªè¨€æ¨¡å‹ã€‚</p></blockquote><p>é™„ä¸Š <code>Transfomers</code> çš„ å®˜æ–¹ç¶²ç«™:</p><p><a href=https://huggingface.co/transformers/>Hugging Face - Transformers</a></p><p>ä»¥åŠ <code>Transformers</code> ç•¶å‰æ‰€æ”¯æ´çš„æ¨¡å‹æ¸…å–®:</p><p><a href=https://huggingface.co/models>Hugging Face - On a mission to solve NLP, one commit at a time.</a></p><hr><p>æˆ‘å°‡ <code>Transformers</code> å¥—ä»¶åˆ†æˆå…©å€‹éƒ¨åˆ†:</p><ol><li><strong>Tokenizer</strong> : å°‡æ–‡å­—æ–·è©å¾Œè½‰æ›ç‚º Index.</li><li><strong>Pre-trained Model</strong> : æ¥å—è½‰æ›çš„ Indexï¼Œ è¼¸å‡º Word Representation.</li></ol><p>ä¸‹é¢é€™å¼µæµç¨‹åœ–ï¼Œæ˜¯å¾æ–‡å­—è½‰æ›æˆ Word Representationçš„éç¨‹ï¼Œæˆ‘å€‘è—‰æ­¤ä¾†äº†è§£é€™å…©å€‹å…ƒä»¶çš„ä½œç”¨(åé»ƒ)åˆ°åº•æ˜¯ä»€éº¼ :</p><p>Tokenizer çš„åŠŸç”¨æ˜¯å°‡æ–‡å­—é€²è¡Œæ–·è©åŠè½‰æ›ç‚ºIndexï¼Œä¸åŒçš„ Pre-trained Language Model æ‰€ä½¿ç”¨çš„æ–·è©æ–¹å¼ä»¥åŠ Index ä¹Ÿä¸ç›¡ç›¸åŒï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨å‰éœ€è¦å…ˆé€²è¡Œã€Œåˆå§‹åŒ–ã€ã€‚</p><pre><code>:::python
from transformers import AutoTokenizer, BertTokenizer

# å¸¸è¦‹æ¨¡å‹å…·æœ‰è‡ªå·±çš„Tokenizer
tokenizer_bert = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)

# AutoTokenizer å‰‡æ˜¯é€šç”¨å‹
tokenizer_other = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
</code></pre><p>é‡å°è¼ƒå¸¸è¦‹çš„èªè¨€æ¨¡å‹ (BERT, GPT2, XLM, XLNet&mldr;)ï¼Œ <code>Transformers</code>æœ‰æä¾›å°ˆå±¬çš„ Tokenizerå¯ä»¥ä½¿ç”¨ã€‚</p><p>é™¤æ­¤ä¹‹å¤–ï¼Œé‚„æœ‰é¡å¤–å®šç¾©ä¸€å€‹ <code>AutoTokenizer</code>ï¼Œå¯ä»¥è®“ä½¿ç”¨è€…äº’ç›¸åˆ†äº«ã€ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ã€‚ é€é <code>tokenizer.from_pretrained(&lt;model_name>)</code>å³å¯ä¸‹è¼‰ä¸¦ä¸”è¼‰å…¥è©²æ¨¡å‹çš„ç›¸é—œè¨­å®šã€‚</p><p>åœ¨åˆæ¬¡ä½¿ç”¨æ¨¡å‹æ™‚ï¼Œ <code>transformers</code>æœƒè‡ªå‹•ä¸‹è¼‰è©²æ¨¡å‹çš„æ¬Šé‡åŠç›¸é—œè¨­å®šï¼Œéœ€è¦èŠ±è²»ä¸€é»æ™‚é–“ (æ™‚é–“é•·åº¦ç«¯çœ‹æ¨¡å‹çš„å¤§å°)ï¼Œä¸‹è¼‰å®Œæˆå¾Œï¼Œå¾€å¾Œçš„åŸ·è¡Œå°‡æœƒå¾æœ¬åœ°ç«¯çš„å¿«å–è³‡æ–™å¤¾è¼‰å…¥ï¼Œå°±ä¸éœ€è¦å†é€²è¡Œä¸‹è¼‰äº†ã€‚</p><pre><code>:::python
tokenizer_bert = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
test_input = &quot;I don't want to work.&quot;

encode_result = tokenizer.encode(test_input)  # ä½¿ç”¨ encode() ä¾†å°æ–‡å­—é€²è¡Œæ–·è©ã€ç·¨ç¢¼ã€‚
print(encode_result)
# [101, 146, 1274, 112, 189, 1328, 1106, 1250, 119, 102]

decode_result = tokenizer.decode(encode_result) # ä½¿ç”¨ decode() ä¾†å°‡ id è½‰æ›å›æ–‡å­—ã€‚
print(decode_result)
# &quot;[CLS] I don't want to work. [SEP]&quot;
</code></pre><p>å¾ä¸­æˆ‘å€‘è§€å¯Ÿåˆ°å…©ä»¶äº‹æƒ…ï¼š</p><ol><li><p><code>encode_result</code> çš„é•·åº¦å¥½åƒè·Ÿ <code>test_input</code> çš„é•·åº¦ä¸ç›¸åŒï¼:</p><p>é€™æ˜¯å› ç‚ºæœ‰äº›æ¨¡å‹æœƒä½¿ç”¨ Word Piece Tokenizerï¼Œä¹Ÿå°±æ˜¯åœ¨æ–·è©éç¨‹ä¸­å°‡è©å½™é€²è¡Œæ‹†è§£ï¼Œæ‰€ä»¥æœƒå°è‡´Encodeéçš„çµæœé•·åº¦èˆ‡åŸå§‹Inputä¸åŒï¼</p></li><li><p>Decodeå¾Œçš„çµæœå¥½åƒè·ŸåŸæœ¬çš„ä¸å¤ªä¸€æ¨£&mldr;:</p><p>å› ç‚ºåœ¨ <code>tokenizer.encode()</code>éç¨‹ä¸­æœƒè‡ªå‹•åŠ å…¥ [CLS]ã€[SEP]ç­‰ç‰¹æ®Šæ¨™è¨˜ï¼Œé€™æ˜¯ç”±æ–¼ BERT æ¨¡å‹çš„æ©Ÿåˆ¶è€Œè‡ªå‹•ç”¢ç”Ÿï¼Œå…¶é¤˜æ¨¡å‹çš„ Tokenizer ä¸ä¸€å®šæœƒæœ‰ï¼Œä¹Ÿå¯ä»¥åœ¨encodeéç¨‹ä¸­åŠ å…¥åƒæ•¸ä¾†ç§»é™¤:</p><pre><code>:::python
encode_result = tokenizer.encode(test_input, add_special_tokens=False) # ç·¨ç¢¼éç¨‹ä¸­ï¼Œç§»é™¤ç‰¹æ®Šç¬¦è™Ÿ
print(encode_result) 
# [146, 1274, 112, 189, 1328, 1106, 1250, 119]

decode_result = tokenizer.decode(encode_result) # ä½¿ç”¨ decode() ä¾†å°‡ id è½‰æ›å›æ–‡å­—ã€‚
print(decode_result)
# &quot;I don't want to work.&quot;
</code></pre></li></ol><p>é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨é€²è¡Œè³‡æ–™å‰è™•ç†æ™‚ï¼Œå¦å¤–ä¸€å€‹éº»ç…©çš„å•é¡Œæ˜¯åœ¨ç”¢ç”Ÿ &ldquo;batch"æ™‚ï¼Œå¿…é ˆè¦è®“æ¯ä¸€ç­†è³‡æ–™çš„é•·åº¦ç›¸åŒ(ä¹Ÿå°±æ˜¯è¦é€²è¡Œ"Padding&rdquo;)ï¼Œå‡è¨­æœ€å¤§é•·åº¦è¨­å®šç‚º <code>64</code>:</p><ul><li>é•·åº¦ä¸è¶³çš„å¥å­éœ€è¦è£œé½Š &ldquo;[PAD]&ldquo;ç¬¦è™Ÿï¼Œä½¿é•·åº¦é”åˆ° 64ã€‚</li><li>é•·åº¦éé•·çš„å¥å­éœ€è¦åªä¿ç•™å‰64å€‹å­—ï¼Œè¶…éçš„éƒ¨åˆ†æ¨æ£„ä¸ç”¨ã€‚</li></ul><p>é€™ä»¶äº‹æƒ…å¦‚æœè‡ªå·±è™•ç†ï¼Œæœƒæœ‰é»å°éº»ç…©ï¼Œå¹¸å¥½ Tokenizer éƒ½å¹«å¿™åšå¥½äº†.</p><pre><code>:::python
test_input = &quot;I don't want to work.&quot;

encode_result = tokenizer.encode(test_input, max_length=64, pad_to_max_length=True)
print(encode_result)
# [101, 146, 1274, 112, 189, 1328, 1106, 1250, 119, 102, 0, 0, 0,.... 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

decode_result = tokenizer.decode(encode_result)
print(decode_result)
# &quot;[CLS] I don't want to work. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]...[PAD]&quot;
</code></pre><p>é€é Tokenizer é€²è¡Œæ–·è©åŠç·¨ç¢¼å¾Œï¼Œæˆ‘å€‘å°±èƒ½å°‡å…¶é€å…¥ Pre-trained Model ä¸­å–å¾— Representationäº†ï¼</p><hr><pre><code>:::python
from transformers import AutoModel, BertModel

model_bert = BertModel.from_pretrained(&quot;bert-base-cased&quot;)
model_other = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
</code></pre><p>èˆ‡ <code>Tokenizer</code>çš„æ©Ÿåˆ¶ç›¸åŒï¼Œ <code>Transformers</code>ä¸­ä¹ŸåŒ…å«äº†å„ç¨®é å…ˆå®šç¾©å¥½çš„ Model (ä¾‹å¦‚ <code>BertModel</code>)ï¼Œä»¥åŠæ–¹ä¾¿ä½¿ç”¨å…¶ä»–é–‹æºæ¨¡å‹çš„æ³›ç”¨Model (<code>AutoModel</code>)ã€‚</p><p>åœ¨ä½¿ç”¨å‰ä¹Ÿéœ€è¦é€é <code>model.from_pretrained(&lt;model_name>)</code>ä¾†æŒ‡å®šè¦è¼‰å…¥çš„æ¨¡å‹ã€‚</p><p>èˆ‡ <code>Tokenizer</code>ç›¸åŒï¼Œåˆæ¬¡ä½¿ç”¨æ¨¡å‹æ™‚æœƒéœ€è¦é€²è¡Œä¸‹è¼‰ï¼Œæ‰€ä»¥éœ€è¦ç­‰å¾…ä¸€æ®µæ™‚é–“ï¼Œä»¥<code>xlnet-large-cased</code>ç‚ºä¾‹ï¼Œå¤§ç´„æœ‰3.3Géœ€è¦é€²è¡Œä¸‹è¼‰ã€‚</p><p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼š<code>Tokenizer</code> åŠ <code>Model</code> æ˜¯æœ‰å…ˆå¾Œé—œä¿‚çš„ï¼š</p><ol><li>æ–‡å­—é€²å…¥<code>Tokenizer</code>è½‰æ›ã€‚</li><li>è½‰æ›å¾Œçš„Indexé€å…¥<code>Model</code>ä¸­å–å¾—Word Representationã€‚</li></ol><p>æ‰€ä»¥ <code>Tokenizer</code> åŠ <code>Model</code> çš„ <code>&lt;model_name></code>å¿…éœ€è¦ä¸€è‡´ï¼Œä»¥å…ç™¼ç”ŸéŒ¯èª¤ã€‚</p><pre><code>:::python
tokenizer_bert = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
test_input = &quot;I don't want to work.&quot;

encode_result = tokenizer.encode(test_input, max_length=64, pad_to_max_length=True)  # ä½¿ç”¨ encode() ä¾†å°æ–‡å­—é€²è¡Œæ–·è©ã€ç·¨ç¢¼ã€‚
print(encode_result)
# [101, 146, 1274, 112, 189, 1328, 1106, 1250, 119, 102, 0, 0, ..., 0]

model_bert = BertModel.from_pretrained(&quot;bert-base-cased&quot;)

output = model_bert(encode_result) # ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.
last_hidden_layer = output[0] # é€™å°±æ˜¯ â€œtest_input&quot; çš„ word representation.

print(last_hidden_layer.shape)
# (1, 64, 768)   =&gt; (batch, words, word vector dimension)
</code></pre><p><code>transformers</code>ä¸­çš„æ‰€æœ‰ <code>model</code> éƒ½æ˜¯ç¹¼æ‰¿ <code>torch.nn.Module</code>è€Œä¾†ï¼Œåœ¨ä½¿ç”¨ä¸Šå…¶å¯¦è·Ÿ Pytorch çš„ Model ç›¸ç•¶é¡ä¼¼ï¼ æ‰€ä»¥åœ¨ä¸Šè¿°çš„èªæ³•ä¸­å°‡ <code>encode_result</code> é€å…¥ <code>model_bert</code>ä¸­ï¼Œå°±ç­‰åŒæ–¼å°‡ <code>encode_result</code>é€å…¥ <code>model</code> ä¸­é€²è¡Œ Forward Propagationï¼Œæœ€å¾Œå³å¯å¾—åˆ° Word Representation.</p><p>è‡³æ–¼ç‚ºä»€éº¼ <code>last_hidden_layer = output[0]</code>ï¼Œå¯ä»¥åƒè€ƒ BertModel çš„ Return æ¬„ä½ï¼Œå›å‚³çš„çµæœæœ‰è¨±å¤šå…§å®¹ï¼Œè€Œç¬¬ä¸€å€‹å°±æ˜¯ã€Œæœ€å¾Œä¸€å±¤çš„ Hidden Statesã€ä¹Ÿå°±æ˜¯æˆ‘å€‘è¦çš„ Word Representation.</p><p>é™„ä¸Š <code>BertModel</code>çš„å®˜æ–¹æ–‡ä»¶é€£çµï¼š
<a href=https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel.forward>BERT - transformers 2.11.0 documentation</a></p><hr><p>äº†è§£ <code>Transformers</code>çš„åŸºæœ¬é‹ä½œåŸç†å¾Œï¼Œå†å›é ­å¼·èª¿ä¸€æ¬¡å®ƒçš„å„ªå‹¢ï¼:</p><ol><li><p><strong>å¯è¦–ç‚ºå°è£å¥½çš„ Language Model :</strong></p><p>Transformers çš„ Model ç¹¼æ‰¿äº† <code>torch.nn.Module</code>ï¼Œä¹Ÿå°±æ˜¯èªªåœ¨è¨­è¨ˆæ•´å€‹ä»»å‹™çš„éç¨‹ä¸­ï¼Œä¸éœ€è¦è‡ªå·±å»å®šç¾© &ldquo;Pre-trained Language Model&rdquo; çš„çµæ§‹ã€ä¹Ÿä¸éœ€è¦è‡ªè¡Œè¼‰å…¥æ¬Šé‡ï¼Œ**åªéœ€è¦å°‡å…¶è¦–ç‚ºä¸€å€‹Pytorch é å…ˆå®šç¾©å¥½çš„ Model ï¼š**å¯¦ä½œä¸€å€‹ <code>Transformers</code>çš„Modelã€è¼‰å…¥ç›¸é—œæ¬Šé‡ï¼Œæ¥ä¸‹ä¾†åªéœ€è¦çµ„è£å°±å¥½äº†ï¼</p><pre><code> :::python
 model_bert = BertModel.from_pretrained(&quot;bert-base-cased&quot;)
 model_ln1 = nn.Linear(768, 3) 

 output = model_bert(encode_result) # ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.
 last_hidden_layer = output[0]
 output = model_ln1(last_hidden_layer) # å¯ä»¥ç›´æ¥å†å°‡ BertModel çš„ Outputé€å…¥å…¶ä»–è‡ªå®šç¾©çš„çµæ§‹ã€‚
</code></pre><p>äº†è§£åˆ°å¦‚ä½•çµ„è£ã€å¦‚ä½•ä½¿ç”¨å¾Œï¼Œé™¤äº†å°‡å…¶æ‹¿ä¾†åš Feature Extractionå¤–ï¼Œä¹Ÿå¯ä»¥ç”¨ç›¸åŒçš„æ¦‚å¿µçµ„è£æˆ Transfer Learning çš„æ¨¡å‹æ¶æ§‹ï¼ä¸éé€™è¶…å‡ºé€™å‰‡ç­†è¨˜æƒ³è¦ç´€éŒ„çš„ç¯„åœäº†ï½</p><p>å¦å¤–ï¼Œç¿’æ…£ä½¿ç”¨ Tensorflow çš„æœ‹å‹ä¹Ÿåˆ¥å‚·å¿ƒï¼Œ <code>transformers</code>ä¹Ÿæœ‰æä¾› tensorflowç‰ˆæœ¬çš„ Model (ä¾‹å¦‚ <code>tfBertModel</code>)!</p></li><li><p><strong>åŒæ¨£çš„æ¶æ§‹èƒ½ä½¿ç”¨å¤šç¨®æ¨¡å‹ â†’ æ–¹ä¾¿æŠ½æ›</strong></p><p>ç”±æ–¼ <code>Transformers</code> ä¸­åŒ…å«è¨±å¤šæ¨¡å‹ï¼šæœ‰éƒ¨åˆ†æ˜¯å®˜æ–¹æŒçºŒæ–°å¢ï¼Œæœ‰äº›å‰‡æ˜¯ä½¿ç”¨è€…äº’ç›¸é‡‹å‡ºï¼Œå°è‡´ä½¿ç”¨é€™å¥—æ¶æ§‹å°±èƒ½å¿«é€Ÿåˆ‡æ›å¤šç¨®æ¨¡å‹ã€‚</p><pre><code> :::python
 MODEL = &quot;bert-base-cased&quot;
 model_language = AutoModel.from_pretrained(MODEL)
 model_ln1 = nn.Linear(768, 3) 

 output = model_language(encode_result) # ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.
 last_hidden_layer = output[0]
 output = model_ln1(last_hidden_layer) # å¯ä»¥ç›´æ¥å†å°‡ model_language çš„ Outputé€å…¥å…¶ä»–è‡ªå®šç¾©çš„çµæ§‹ã€‚
</code></pre><p>ç›¸åŒçš„ç¨‹å¼ç¢¼ï¼Œåªéœ€è¦æ›´æ›æ¨¡å‹è®Šæ•¸å³å¯é”åˆ°ã€ŒæŠ½æ›åº•å±¤ Pre-trained Langauge Modelçš„æ•ˆæœã€</p><pre><code> :::python
 MODEL = &quot;xlnet-large-cased&quot;
 model_language = AutoModel.from_pretrained(MODEL)
 model_ln1 = nn.Linear(768, 3) 

 output = model_language(encode_result) # ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.
 last_hidden_layer = output[0]
 output = model_ln1(last_hidden_layer) # å¯ä»¥ç›´æ¥å†å°‡ model_language çš„ Outputé€å…¥å…¶ä»–è‡ªå®šç¾©çš„çµæ§‹ã€‚
</code></pre></li></ol><hr><p>æ­¤å¤–ï¼Œåœ¨Surveyæ–°çš„Language Modelæ™‚ï¼Œå¦‚æœåœ¨Githubä¸Šé¢çœ‹åˆ°å®ƒæ”¯æ´ <code>transformers</code>ï¼Œæœƒå‚™æ„Ÿæ¬£æ…°QQï¼Œé “æ™‚æ„Ÿåˆ°ä¸–ç•Œçš„ç¾å¥½ï½</p><p>æ„Ÿè¬ä½ çš„é–±è®€ï¼Œå¸Œæœ›ä½ ä¹Ÿèƒ½è·Ÿæˆ‘ä¸€èµ·æ„Ÿå—ä¸–ç•Œçš„ç¾å¥½ï¼XD</p><p>æœ‰ä»»ä½•å•é¡Œæ­¡è¿ä¸€èµ·äº¤æµï¼ ä¸‹æ¬¡è¦‹ï¼</p><hr><ul class=pager><li class=previous><a href=/notes/2020/20200527.html/ data-toggle=tooltip data-placement=top title=ä¸å†è¢«å¤§é‡è¶…åƒæ•¸åŠæ¨¡å‹è¡¨ç¾æ·¹æ²’ï¼ä½¿ç”¨MLFlowé€²è¡Œå¯¦é©—ç®¡ç†>&larr;
Previous Post</a></li><li class=next><a href=/notes/2020/gitlab-ci-cd.html/ data-toggle=tooltip data-placement=top title="é€²è·å ´å­¸åˆ°çš„Git - (1) Gitlab CI/CD">Next
Post &rarr;</a></li></ul></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/ansible title=ansible>ansible</a>
<a href=/tags/fast-api title=fast-api>fast-api</a>
<a href=/tags/python title=python>python</a>
<a href=/tags/test title=test>test</a>
<a href=/tags/tool title=tool>tool</a>
<a href=/tags/wsgi title=wsgi>wsgi</a></div></section></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:alen6997535@gmail.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/minglunwu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://www.linkedin.com/in/ming-lun-wu-637020142/><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-linkedin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://medium.com/@minglun-wu><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-medium fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; MingLun Blog 2023<br><a href=https://themes.gohugo.io/hugo-theme-cleanwhite>CleanWhite Hugo Theme</a> by <a href=https://zhaohuabing.com>Huabing</a> |
<iframe style=margin-left:2px;margin-bottom:-5px frameborder=0 scrolling=0 width=100px height=20px src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true"></iframe></p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){t=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),n=$(this).text(),i=$('<a href="'+o+'" rel="nofollow">'+n+"</a>"),s=$('<li class="'+t+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>