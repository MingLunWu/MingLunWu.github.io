<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:site_name" content="MingLun Blog"><meta property="og:type" content="article"><meta property="og:image" content="https://images.unsplash.com/photo-1502951682449-e5b93545d46e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1566&q=80"><meta property="twitter:image" content="https://images.unsplash.com/photo-1502951682449-e5b93545d46e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1566&q=80"><meta name=title content="從零開始 - Pytorch 入門懶人包"><meta property="og:title" content="從零開始 - Pytorch 入門懶人包"><meta property="twitter:title" content="從零開始 - Pytorch 入門懶人包"><meta name=description content><meta property="og:description" content><meta property="twitter:description" content><meta property="twitter:card" content="研究所時有花時間去了解Neural Network的概念，但卻一直沒有機會進行實作，最近有機會可以從頭開始學習Pytorch，把學習的過程整理記錄下來，希望想要快速上手Pytorch的人，可以透過這篇文章快速入門！"><meta name=keyword content="吳明倫, MingLun, minglunwu"><link rel="shortcut icon" href=/img/favicon.ico><title>從零開始 - Pytorch 入門懶人包 | MingLun Blog</title><link rel=canonical href=/notes/2020/20200324.html/><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/hugo-theme-cleanwhite.min.css><link rel=stylesheet href=/css/zanshang.css><link rel=stylesheet href=/css/font-awesome.all.min.css><script src=/js/jquery.min.js></script>
<script src=/js/bootstrap.min.js></script>
<script src=/js/hux-blog.min.js></script>
<script src=/js/lazysizes.min.js></script></head><nav class="navbar navbar-default navbar-custom navbar-fixed-top"><div class=container-fluid><div class="navbar-header page-scroll"><button type=button class=navbar-toggle>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button>
<a class=navbar-brand href=/>MingLun Blog</a></div><div id=huxblog_navbar><div class=navbar-collapse><ul class="nav navbar-nav navbar-right"><li><a href=/>All Posts</a></li><li><a href=/archive//>ARCHIVE</a></li><li><a href=/notes//>NOTES</a></li><li><a href=/about//>ABOUT</a></li><li><a href=/search><i class="fa fa-search"></i></a></li></ul></div></div></div></nav><script>var $body=document.body,$toggle=document.querySelector(".navbar-toggle"),$navbar=document.querySelector("#huxblog_navbar"),$collapse=document.querySelector(".navbar-collapse");$toggle.addEventListener("click",handleMagic);function handleMagic(){$navbar.className.indexOf("in")>0?($navbar.className=" ",setTimeout(function(){$navbar.className.indexOf("in")<0&&($collapse.style.height="0px")},400)):($collapse.style.height="auto",$navbar.className+=" in")}</script><style type=text/css>header.intro-header{background-image:url(https://images.unsplash.com/photo-1502951682449-e5b93545d46e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1566&q=80)}</style><header class=intro-header><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><div class=tags><a class=tag href=/tags/tool title=Tool>Tool</a></div><h1>從零開始 - Pytorch 入門懶人包</h1><h2 class=subheading></h2><span class=meta>Posted by
MingLun Allen Wu
on
Tuesday, March 24, 2020</span></div></div></div></div></header><article><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
post-container"><h1 id=前言>前言</h1><p>研究所時有花時間去了解Neural Network的概念，但卻一直沒有機會進行實作，最近有機會可以從頭開始學習Pytorch，把學習的過程整理記錄下來，希望想要快速上手Pytorch的人，可以透過這篇文章快速入門！</p><h1 id=目錄>目錄</h1><ol><li><a href=#%E5%89%8D%E8%A8%80>前言</a></li><li><a href=#%E7%9B%AE%E9%8C%84>目錄</a></li><li><a href=#Tensor%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E3%80%81%E6%A0%BC%E5%BC%8F%E8%BD%89%E6%8F%9B>Tensor的基本使用、格式轉換</a></li><li><a href=#%E5%B8%B8%E6%9C%83%E4%BD%BF%E7%94%A8%E5%88%B0%E7%9A%84Module>常會使用到的Module</a></li><li><a href=#%E5%BB%BA%E7%AB%8B%E4%B8%80%E5%80%8BNetwork>建立一個Network</a></li><li><a href=#%E5%BB%BA%E7%AB%8B%E8%A8%93%E7%B7%B4%E8%B3%87%E6%96%99%E9%9B%86>建立訓練資料集</a></li><li><a href=#%E8%A8%93%E7%B7%B4%E7%9A%84Pipeline>訓練的Pipeline</a></li><li><a href=#%E8%A6%96%E8%A6%BA%E5%8C%96%E5%B7%A5%E5%85%B7-TensorBoard>視覺化工具-TensorBoard</a></li><li><a href=#%E5%84%B2%E5%AD%98%E3%80%81%E8%BC%89%E5%85%A5Model>儲存、載入Model</a></li><li><a href=#%E4%BD%BF%E7%94%A8GPU>使用GPU</a></li></ol><hr><h1 id=tensor的基本使用格式轉換>Tensor的基本使用、格式轉換</h1><h2 id=建立tensor>建立Tensor</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>a <span style=color:#ff79c6>=</span> [<span style=color:#bd93f9>1</span>,<span style=color:#bd93f9>2</span>,<span style=color:#bd93f9>3</span>]
</span></span><span style=display:flex><span>tensor_a <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>tensor(a)
</span></span></code></pre></div><h2 id=tensor-的維度轉換>Tensor 的維度轉換</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tensor_reshape <span style=color:#ff79c6>=</span> tensor_a<span style=color:#ff79c6>.</span>view(<span style=color:#bd93f9>1</span>,<span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>)
</span></span></code></pre></div><h2 id=tensor-中的元素型態轉換>Tensor 中的元素型態轉換</h2><p>僅需要在tensor之後加上轉換的型態即可。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tensor_a_long <span style=color:#ff79c6>=</span> tensor_a<span style=color:#ff79c6>.</span>long() <span style=color:#6272a4># 將tensor_a轉換為long資料型態。</span>
</span></span><span style=display:flex><span>tensor_a_float <span style=color:#ff79c6>=</span> tensor_a<span style=color:#ff79c6>.</span>float() <span style=color:#6272a4># 將tensor_a轉換為float資料型態。</span>
</span></span></code></pre></div><h2 id=與其他常用套件之轉換>與其他常用套件之轉換</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tensor_b <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>from_numpy(np_element) <span style=color:#6272a4># numpy -&gt; torch</span>
</span></span><span style=display:flex><span>np_a <span style=color:#ff79c6>=</span> tensor_a<span style=color:#ff79c6>.</span>numpy() <span style=color:#6272a4># torch -&gt; numpy</span>
</span></span></code></pre></div><hr><h1 id=常會使用到的module>常會使用到的Module</h1><p>通常在Pytorch時，常會使用到下列的Module</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> torch
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torch.nn <span style=color:#ff79c6>as</span> nn
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torch.nn.functional <span style=color:#ff79c6>as</span> F
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torch.optim <span style=color:#ff79c6>as</span> optim
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> torch.utils.data <span style=color:#ff79c6>import</span> Dataset, DataLoader
</span></span><span style=display:flex><span><span style=color:#ff79c6>from</span> torch.utils.tensorboard <span style=color:#ff79c6>import</span> SummaryWriter
</span></span></code></pre></div><ul><li><strong>torch</strong> : Pytorch基本的套件</li><li><strong>torch.nn</strong> : 定義了基本的Layer元件 (例如: Linear)，在建構模型時會使用到。(請見<a href=#%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%8B%E4%B8%80%E5%80%8BNetwork>5. 如何建立一個Network。</a>)</li><li><strong>torch.nn.functional</strong> : 定義了卷積、Activation Function等。</li><li><strong>torch.optim</strong> : 定義了許多常見的optimizer.（請見<a href=#%E8%A8%93%E7%B7%B4%E7%9A%84Pipeline>7.訓練的Pipeline</a>)</li><li><strong>torch.utils.data</strong> : 定義了Dataset及DataLoader等資料相關Class (請見<a href=#%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%8B%E8%A8%93%E7%B7%B4%E8%B3%87%E6%96%99%E9%9B%86>6. 如何建立訓練資料集</a>)</li><li><strong>torch.utils.tensorboard</strong> : 定義了與Tensorboard互動相關的Class (請見<a href=#%E8%A6%96%E8%A6%BA%E5%8C%96%E5%B7%A5%E5%85%B7-TensorBoard>8. 視覺化工具-TensorBoard</a>)</li></ul><hr><h1 id=建立一個network>建立一個Network</h1><p>通常透過Pytorch 建立一個Network時，我們習慣透過定義<strong>Python的Class</strong> 來建構我們的Network. 這一個Python Class必須具備下列特性:</p><ol><li><p>必須繼承 <code>torch.nn.Module</code>，這樣才能使用Pytorch內建的各種function，並且與其他Pytorch元件進行互動。</p></li><li><p>繼承 <code>torch.nn.Module</code>後，會需要Override一些特定的Method:</p><ul><li><strong>_<em>init</em>_()</strong>:</li></ul><p>定義在Initial此Class(Network)時需要初始化的元件。基本上在Network中需要使用到的Layer、Hyperparameter都需要在這邊先行定義。</p><ul><li><strong>forward()</strong>:<ul><li><p>透過Override forward這個Method來定義此Network的Forward Propagation方式。</p></li><li><p>值得注意的是<code>forward()</code> 在Override以後，往後透過可以直接透過call model來進行forward(). (請看下方範例)</p></li></ul></li></ul></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>MyOwnNet</span>(nn<span style=color:#ff79c6>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> __init__(self):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>(my_own_net, self)<span style=color:#ff79c6>.</span>__init__() <span style=color:#6272a4>#初始化 nn.Module class</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 以下按照需求定義Layer.</span>
</span></span><span style=display:flex><span>        self<span style=color:#ff79c6>.</span>ln <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Linear(<span style=color:#bd93f9>768</span>, <span style=color:#bd93f9>384</span>)
</span></span><span style=display:flex><span>        self<span style=color:#ff79c6>.</span>ln_2 <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Linear(<span style=color:#bd93f9>384</span>, <span style=color:#bd93f9>10</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>forward</span>(self, x):
</span></span><span style=display:flex><span>        outputs <span style=color:#ff79c6>=</span> self<span style=color:#ff79c6>.</span>ln(x)
</span></span><span style=display:flex><span>        outputs <span style=color:#ff79c6>=</span> self<span style=color:#ff79c6>.</span>ln_2(outputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>if</span> __name__ <span style=color:#ff79c6>==</span> <span style=color:#f1fa8c>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    net <span style=color:#ff79c6>=</span> MyOwnNet()
</span></span><span style=display:flex><span>    test <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>list</span>(<span style=color:#8be9fd;font-style:italic>range</span>(<span style=color:#bd93f9>768</span>))
</span></span><span style=display:flex><span>    test_input <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>tensor(test)
</span></span><span style=display:flex><span>    outputs <span style=color:#ff79c6>=</span> net(test_input) <span style=color:#6272a4># 直接呼叫instance，將會自動執行forward()的function.</span>
</span></span></code></pre></div><hr><h1 id=建立訓練資料集>建立訓練資料集</h1><p>在定義好模型架構後，接著需要處理資料的部分。</p><p>當然在提供訓練資料時，我們也可以單純透過迴圈的方式自行提取，但是透過Pytorch的資料集，我們不需要再特別去處理「Batch size」或是「Shuffle」的問題。</p><p>這裡記錄了兩種Pytorch 內建的Module提供我們實作並且改寫:</p><ol><li>Dataset</li><li>DataLoader</li></ol><h2 id=dataset>Dataset</h2><p>實作時，與上節建立Network相同，需要先定義一個Python Class來繼承 <code>torch.utils.data.Dataset</code>，並且要Override以下Methods:</p><ul><li><p><strong>_<em>init</em>_(self)</strong>: 初始化instance時需要進行的動作，通常會在這個地方載入資料集、或是進行前處理。</p></li><li><p><strong>_getitem(self, index)__</strong>: 定義使用idx去query元素時要進行的動作。 (通常直接回傳第index筆資料)</p></li><li><p><strong>_len(self)__</strong>: 定義使用len()去取得instance元素數量時要進行的動作。 （通常直接回傳資料筆數)</p><p>:::python
class OwnDataset(Dataset):
def <strong>init</strong>(self, file_path):
super(OwnDataset, self).<strong>init</strong>()
self.data = pickle.load(open(file_path, &ldquo;rb&rdquo;)) # 讀取資料</p><pre><code>  def __len__(self):
      return len(self.data)

  def __getitem__(self, idx):
      return {&quot;x&quot;: self.data[&quot;feature&quot;], &quot;y&quot;: self.data[&quot;label&quot;]}
</code></pre><p>if <strong>name</strong> == &ldquo;<strong>main</strong>&rdquo;:
dataset = OwnDataset("./data/test.pickle") #Initial an instance.</p><pre><code>  print(len(dataset)) # Call __len__()
  a_example = dataset[0] # Call __getitem__()

  feature, label = a_example[&quot;x&quot;], a_example[&quot;y&quot;]
</code></pre></li></ul><p>從範例中可以看到使用Dataset的好處在於先行定義好回傳資料的格式、以及如何取用資料。 進行訓練時就不需要再重複的撰寫取用資料的程式。</p><p>也可以在Dataset中加入一個 <code>type</code>變數來切換要回傳 training、evaluation、testing set. 並且針對傳入的型態不同來進行資料的Sample。</p><h2 id=dataloader>DataLoader</h2><p>除此之外，再進行訓練時常會需要動態的調整 <code>batch_size</code>以及需要打亂資料(Shuffle)，如果自行撰寫Function的話，常會被index搞得昏頭轉向。 有時多一個idx就會造成 out of range的錯誤。</p><p>此時如果你有按照上述的格式定義好一個 <code>Dataset</code>，那麼以上任務都不用擔心，我們可以透過 <code>DataLoader</code>直接處理好。</p><p><code>DataLoader</code>具有幾個參數:</p><ul><li>dataset: 放入我們剛剛創建的OwnDataset Instance.</li><li>batch_size: 一個batch要包含多少資料筆數。</li><li>shuffle: 是否對資料進行隨機調整。</li><li>num_workers: 透過Multi-Process來加速資料的取用，避免訓練時速度被IO給限制。（不建議使用在GPU環境)</li><li>pin_memory: 在使用GPU時，啟用此屬性能提升訓練速度。</li></ul><p>有關<code>num_workers</code>, <code>pin_memory</code>的探討，建議可以參考<a href=https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading>官方文件</a></p><pre><code>:::python
data_loader = DataLoader(dataset= dataset, batch_size= 4, shuffle= True, num_workers= 2, pin_memory= True)

print(len(data_loader)) #回傳當前共有幾個batch，可以直接用這個數值來作為Step.

data_iter = iter(data_loader)
x, y = data_iter.next() # 透過這種方式來取用資料。

for (x,y) in data_iter: # 也可以透過For loop來取用資料。
    some_train_step(x)
</code></pre><p>在定義完 <code>Dataset</code>後，透過 <code>DataLoader</code>來對資料進行訓練前的處理，接著就能按照需求去取得資料。相當的方便且簡潔。</p><hr><h1 id=訓練的pipeline>訓練的Pipeline</h1><p>個人認為建構Model的Pipeline大略如下:</p><ol><li>定義Model.</li><li>定義Dataset.</li><li>定義Loss以及Optimizer.</li><li>進行訓練.</li></ol><p>其中第一點以及第二點請參考本文前段。</p><h2 id=定義loss及optimizer>定義Loss及Optimizer</h2><p>在 <code>torch.nn</code> 以及 <code>torch.nn.Functional</code>中定義了許多不同的Loss Function，可以根據需求自行選擇. 以下範例以分類問題的CrossEntropy為例:</p><pre><code>:::python
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Init an instance.
criterion = nn.CrossEntropyLoss() 
# 建立一個optimizer來優化 model 的所有&quot;可訓練參數&quot;
optimizer = optim.Adam(model.parameters(), lr=1e-5)
</code></pre><p>首先，必須先建立計算Loss以及Optimizer的Instance。</p><p>在建立Optimizer時會需要設定優化的對象，通常會直接放<code>model.parameters()</code>，代表<code>model</code>中所有可訓練的參數。而不同的Optimizer(SGD, Adam, &mldr;)會有不同的參數要進行設定。</p><h2 id=進行訓練>進行訓練</h2><pre><code>:::python
feature, label = data_iter.next() #透過前面提到的iterator取得一個batch的資料。

outputs = model(x) # 將訓練資料送入model中進行forward propagation。

loss = criterion(outputs, y) # 回傳當前Forward結果與真實Label的Loss

optimizer.zero_grad() # 先清空當前的梯度值
loss.backward() # 進行Backward Propagation
optimizer.step() # 針對Backward Propagation所得到的梯度調整參數。
</code></pre><p>接著直接呼叫<code>model(x)</code>如同上節所說，就是直接將<code>x</code>送入<code>model</code>中進行Forward Propagation。 得到的結果可以直接與真實label送到剛剛建立的Loss Instance計算Loss.</p><p>在計算完Loss後，我們就能直接使用 <code>loss.backward()</code>來取得Loss對所有參數的梯度。 在Pytorch中，我們只有定義Forward的方式，而Backward Propagation只需要透過短短一行即可得到。</p><p>取得每一個參數的梯度以後，就能呼叫剛剛定義的<code>optimizer.step()</code>來進行參數調整。</p><p>以上就是一次的訓練迭代:
<strong>Forward propagation -> 計算Loss -> Backward propagation -> Optimize (根據梯度調整Weight.)</strong></p><p>實際訓練時可根據需求來不斷從Data Iterator中取得資料，重複上述迭代進行訓練，也因為會有不斷的迭代，所以記得使用<code>optimizer.zero_grad()</code>來清空上一次的梯度。</p><p>另外，在Pytorch中的Tensor都會有<code>requires_grad</code>的屬性，如果啟用的話會自動追蹤計算圖，方便直接呼叫<code>backward()</code>，如果不希望啟用的話，可以透過下列方法解除:</p><pre><code>:::python
# Method 1
tensor_nograd = torch.tensor([1,2,3], requires_grad=False)

# Method 2
with torch.no_grad(): # 以下做的事情都不會取得梯度。
    # Your Code.
</code></pre><hr><h1 id=視覺化工具-tensorboard>視覺化工具-TensorBoard</h1><h2 id=基本使用>基本使用</h2><p>在訓練的過程中，我們需要觀察Loss或是Accuracy來確認訓練的效果，雖然可以透過Print Log的方式來顯示，但其實透過有更好的工具能夠協助視覺化。</p><pre><code>:::python
from torch.utils.tensorboard import SummaryWriter

LOGDIR = &quot;./logs/&quot; # Define 資料要被寫入的位置
writer = SummaryWriter(LOGDIR)

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
</code></pre><p>也就是在定義了一個writer後，可以透過<code>writer.add_scalar</code>的方式將想要觀測的值記錄下來，同時可以分門別類的設定標籤、Step或是Epoch數目。 除了<code>add_scalar</code>外，還有許多如<code>add_image</code>、<code>add_graph</code>的方法可以使用。</p><p>寫入資料後，執行tensorboard即可在LocalHost的瀏覽器觀察結果:</p><pre><code>:::bash
pip install tensorboard
tensorboard --logdir=&quot;./logs&quot;
</code></pre><p><img src=https://pytorch.org/docs/stable/_images/hier_tags.png alt></p><p><a href=https://pytorch.org/docs/stable/tensorboard.html>圖片來源</a></p><h2 id=remote-server>Remote Server</h2><p>另外在進行機器學習時，常常會需要使用到遠端主機的GPU，紀錄一下如何在Localhost查看遠端機器的訓練狀態。 首先還是一樣要在訓練過程中透過<code>SummaryWriter</code>將log寫入。</p><p>接著透過 SSH連線將本地端的一個Port 與 遠端機器的特定Port綁定在一起，首先在本地端執行:</p><pre><code>:::bash
ssh -L 16001:127.0.0.1:16001 username@server_ip
</code></pre><p>透過特定Port與遠端主機連線。</p><p>接著在 <strong>遠端主機</strong>執行</p><pre><code>:::bash
tensorboard --logdir=&quot;./logs&quot; --port=16001
</code></pre><p>同樣的啟動指令，只是規定要在剛剛設定Port上啟動服務。</p><p>如此一來就能在自己的主機上查看遠端Server的訓練狀況了。</p><hr><h1 id=儲存載入model>儲存、載入Model</h1><p>在訓練完模型後，需要將模型儲存下來，方便日後驗證或使用。
Pytorch 提供了兩種儲存方法: <code>完整模型</code>以及 <code>State_dict</code></p><h2 id=完整模型>完整模型:</h2><p>官方較不推薦這種方式，由於是透過<code>pickle</code>的方式進行儲存，很可能會遭遇意料之外的問題。</p><h3 id=save>Save</h3><pre><code>:::python
torch.save(model, PATH)
</code></pre><h3 id=load>Load</h3><pre><code>:::python
model = torch.load(PATH)
model.eval()
</code></pre><p>在Pytorch的model中，可以透過 <code>model.train()</code>以及<code>model.eval()</code>來切換不同模式，使用<code>model.eval()</code>會將dropout layer 以及 batch_normalization 切換成驗證模式，避免在Inference的過程中造成結果不一致。</p><h2 id=state-dict>State Dict</h2><p>State Dictionary 則是透過 <code>Python的Dictionary</code>來儲存每一層的內容以及權重。如果要查看的話可以透過<code>model.state_dict()</code>來取得。</p><h3 id=save-1>Save</h3><pre><code>:::python
torch.save(model.state_dict(), PATH)
</code></pre><h3 id=load-1>Load</h3><pre><code>:::python
model = MyOwnNet() # 要先建立相同的Class Instance.
model.load_state_dict(torch.load(PATH))
model.eval()
</code></pre><hr><h1 id=使用gpu>使用GPU</h1><p>在上述的內容完成後，基本上已經可以建立一個簡易的Neural Network了，接下來紀錄如何將快速的將資料從CPU訓練切換為GPU.</p><p>首先要先確定自己的Pytorch是有安裝到CUDA版本。
可以透過下列指令確認:</p><pre><code>:::python
print(torch.cuda.is_available())
</code></pre><p>如果是True則代表有成功偵測到GPU，若為False則可能是設定錯誤！</p><p>接著要建立一個device變數:</p><pre><code>:::python
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
</code></pre><p>接著就是將自己的Model以及需要送進Model的Input都轉換為GPU模式.</p><pre><code>:::python
model = model.to(device)
x, y = x.to(device), y.to(device)
# 以下正常進行使用
</code></pre><p>只要加上短短一行指令即可切換為GPU模式，此時如果在print這些tensor，可以發現數值不變，但是後面多了一個"cuda:n"屬性，這就代表Tensor已經被送到GPU去了。</p><hr><h1 id=後記>後記</h1><p>在學習Pytorch的過程中，很多教材都是語法居多，透過實際進行任務的方式教學，但我在過程中對於許多Component都似懂非懂，現在稍微釐清後，記錄下來，希望如果是想學習Pytorch的入門者，看完這篇文章可以了解一些基本觀念，在看網路上的Tutorial或是Track別人的Code時，能夠不再霧煞煞～</p><hr><ul class=pager><li class=previous><a href=/notes/2020/20200318/ data-toggle=tooltip data-placement=top title="安裝 Miniconda 以及 Pytorch">&larr;
Previous Post</a></li><li class=next><a href=/notes/2020/20200416.html/ data-toggle=tooltip data-placement=top title="Pytorch Lightning 入門筆記">Next
Post &rarr;</a></li></ul></div><div class="col-lg-2 col-lg-offset-0
visible-lg-block
sidebar-container
catalog-container"><div class=side-catalog><hr class="hidden-sm hidden-xs"><h5><a class=catalog-toggle href=#>CATALOG</a></h5><ul class=catalog-body></ul></div></div><div class="col-lg-8 col-lg-offset-2
col-md-10 col-md-offset-1
sidebar-container"><section><hr class="hidden-sm hidden-xs"><h5><a href=/tags/>FEATURED TAGS</a></h5><div class=tags><a href=/tags/ansible title=ansible>ansible</a>
<a href=/tags/fast-api title=fast-api>fast-api</a>
<a href=/tags/python title=python>python</a>
<a href=/tags/test title=test>test</a>
<a href=/tags/tool title=tool>tool</a>
<a href=/tags/wsgi title=wsgi>wsgi</a></div></section></div></div></div></article><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center"><li><a href=mailto:youremail@gmail.com><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=/your%20wechat%20qr%20code%20image><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-weixin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://github.com/yourgithub><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://www.linkedin.com/in/yourlinkedinid><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-linkedin fa-stack-1x fa-inverse"></i></span></a></li><li><a target=_blank href=https://stackoverflow.com/users/yourstackoverflowid><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-stack-overflow fa-stack-1x fa-inverse"></i></span></a></li><li><a href rel=alternate type=application/rss+xml title="MingLun Blog"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="copyright text-muted">Copyright &copy; MingLun Blog 2023<br><a href=https://themes.gohugo.io/hugo-theme-cleanwhite>CleanWhite Hugo Theme</a> by <a href=https://zhaohuabing.com>Huabing</a> |
<iframe style=margin-left:2px;margin-bottom:-5px frameborder=0 scrolling=0 width=100px height=20px src="https://ghbtns.com/github-btn.html?user=zhaohuabing&repo=hugo-theme-cleanwhite&type=star&count=true"></iframe></p></div></div></div></footer><script>function loadAsync(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(null,e)},!1),i.parentNode.insertBefore(n,i)}</script><script>$("#tag_cloud").length!==0&&loadAsync("/js/jquery.tagcloud.js",function(){$.fn.tagcloud.defaults={color:{start:"#bbbbee",end:"#0085a1"}},$("#tag_cloud a").tagcloud()})</script><script>loadAsync("https://cdn.jsdelivr.net/npm/fastclick@1.0.6/lib/fastclick.min.js",function(){var e=document.querySelector("nav");e&&FastClick.attach(e)})</script><script type=text/javascript>function generateCatalog(e){_containerSelector="div.post-container";var t,n,s,o,i,a=$(_containerSelector),r=a.find("h1,h2,h3,h4,h5,h6");return $(e).html(""),r.each(function(){t=$(this).prop("tagName").toLowerCase(),o="#"+$(this).prop("id"),n=$(this).text(),i=$('<a href="'+o+'" rel="nofollow">'+n+"</a>"),s=$('<li class="'+t+'_nav"></li>').append(i),$(e).append(s)}),!0}generateCatalog(".catalog-body"),$(".catalog-toggle").click(function(e){e.preventDefault(),$(".side-catalog").toggleClass("fold")}),loadAsync("/js/jquery.nav.js",function(){$(".catalog-body").onePageNav({currentClass:"active",changeHash:!1,easing:"swing",filter:"",scrollSpeed:700,scrollOffset:0,scrollThreshold:.2,begin:null,end:null,scrollChange:null,padding:80})})</script></body></html>