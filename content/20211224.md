title: 資料品質為什麼很重要? 如何監控資料品質?
authors: MingLun Allen Wu
date: 2021-12-24 12:00:00
tags: 待定
category: 待定
summary: 待定
slug: data_quality
top_image: https://images.unsplash.com/photo-1488229297570-58520851e868?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1469&q=80

# TL;DR

在資料分析的領域中，資料品質有點像是「溫室效應」: **大家都知道它很重要，卻不是每個人都想花時間處理，然後有一天，就Fucked Up了**。

近期試著在部門建立監控資料品質的機制，透過這篇文章記錄過程中建立的知識點，以及最後是怎麼實作出可套用在現行數百個 ETL 的監控模組。

# 到底哪裡出了問題?

大約半年前，我正在前一份工作建模，在確定了基本的模型架構後，使用手邊現有的資料進行預測，結果相當不錯! 團隊歡欣鼓舞。

到了下個月，新的資料收集完畢，丟進一樣的模型做預測，結果**慘不忍睹**，團隊成員面面相覷。

欸! 你有改模型的架構嗎?

沒...沒有阿...?

會不會這個月的資料本來就怪怪的啊...?

我...我也不知道欸...

<img style="display:block; margin-left:auto; margin-right:auto; width:100%;" src="https://minglunwu.github.io/images/20211224/why.png">


模型的架構，可以透過**程式碼版控**去追蹤每個版本的更動。

而每一批資料的變動是否有異常的情況發生，這就需要花點工夫深入了解了。

# 對建模的迷思

剛踏入資料分析的領域時，有種迷思:

> 預測結果不好，一定是這個模型架構太爛了!

所以花了很多時間去追最新的模型架構、研讀新的論文，陷入了一種「套件工程師」的感覺: **把相同的資料，丟進不同的模型試試，如果表現不好，就再換一種模型**。

直到上了 Andrew Ng. 的 Coursera 課程 - <a href="https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops?#courses">MLOps Specialization</a>

其中分享了一個觀念: `Model-Centric` 與 `Data-Centric` AI 的不同，分別透過`改善模型`及`改善資料`來達到優化結果的目的。

在 Andrew 公司內部的多個專案中，他們發現`改善資料`的改善幅度是遠大於`改善模型`的。要做到`改善資料`，勢必要深入的瞭解資料。

如果有興趣的讀者，也可以看看以下影片: 

<iframe width="560" height="315" src="https://www.youtube.com/embed/06-AZXmwHjo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

此外，當模型上線運行後，一但Inference資料的分布發生異常，很容易造成模型表現一落千丈，這種情況常被稱為 `Data Drift` (資料偏移)，透過計算資料品質，能夠盡早發現這樣的狀況。

# 資料品質


