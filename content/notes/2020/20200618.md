---
title: NLPæ·±åº¦å­¸ç¿’ä¸èƒ½ä¸ç”¨çš„å¥—ä»¶ - Transformers
author: MingLun Allen Wu
date: 2020-06-18
tags: 
    - transformers
category: []
summary: é€éTransformerså¥—ä»¶ä½¿ç”¨å¤šç¨®Pre-trained Language Modelï¼Œä¸¦ä¸”å¯¦ä½œText Feature Extraction.
slug: "transformers-tutorial.html"
image: https://images.unsplash.com/photo-1554631995-6c22c6e729fe?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=733&q=80
---
- [å‰è¨€](#å‰è¨€)
- [å¦‚ä½•ä½¿ç”¨Transformers](#å¦‚ä½•ä½¿ç”¨transformers)
  - [Tokenizer](#tokenizer)
    - [1. åˆå§‹åŒ–](#1-åˆå§‹åŒ–)
    - [2. åŸºæœ¬ä½¿ç”¨](#2-åŸºæœ¬ä½¿ç”¨)
  - [Pre-trained Language Model](#pre-trained-language-model)
    - [1. åˆå§‹åŒ–](#1-åˆå§‹åŒ–-1)
    - [2. åŸºæœ¬ä½¿ç”¨](#2-åŸºæœ¬ä½¿ç”¨-1)
- [Transformers çš„å„ªå‹¢](#transformers-çš„å„ªå‹¢)
- [å¾Œè¨˜](#å¾Œè¨˜)
  
<p id="å‰è¨€"></p>
# å‰è¨€

æœ¬ç¯‡ç­†è¨˜æ‰€ç´€éŒ„çš„ Transformers ä¸¦ä¸æ˜¯ Attention è«–æ–‡ä¸­æ‰€æåˆ°çš„ "Transformer"æ¨¡å‹ï¼Œè€Œæ˜¯ç”±Higginfaceåœ˜éšŠæ‰€é–‹ç™¼çš„ `Transformers` å¥—ä»¶ã€‚

é€™å‰‡ç­†è¨˜çš„é‡é»åœ¨æ–¼:  

1. ç‚ºä»€éº¼è¦ä½¿ç”¨ Transformers
2. Transformers çš„å…©å¤§å…ƒä»¶:
    - Tokenizer
    - Model
3. ä½¿ç”¨ Transformers å¯¦ä½œ Feature Extraction.

è¿‘å¹´ä¾† NLP åœ¨ Attention æ¦‚å¿µæå‡ºä»¥å¾Œï¼Œå„ç¨®æ¨¡å‹å¦‚åŒé›¨å¾Œæ˜¥ç­èˆ¬å™´ç™¼ï¼š
<img style="display:block; margin-left:auto; margin-right:auto; width:100%;" src="https://miro.medium.com/max/1400/1*corMthPJwan-yw0KOcZ6qQ.png">
(åœ–ç‰‡ä¾†æº: [https://medium.com/@hamdan.hussam/from-bert-to-albert-pre-trained-langaug-models-5865aa5c3762](https://medium.com/@hamdan.hussam/from-bert-to-albert-pre-trained-langaug-models-5865aa5c3762))

ç„¶è€Œåœ¨ä½¿ç”¨ä¸åŒçš„æ¨¡å‹æ™‚ï¼Œæ¯å€‹æ¨¡å‹çš„æ¶æ§‹ã€åƒæ•¸çš„è¼‰å…¥æ–¹å¼éƒ½ä¸ç›¸åŒï¼Œå¤§å¤§çš„æé«˜äº†æ¨¡å‹é–“çš„è½‰æ›æˆæœ¬ã€‚èˆ‰ä¾‹ä¾†èªªï¼š é€²è¡Œå‡æ–°èåˆ†é¡ä»»å‹™æ™‚ï¼Œå¯èƒ½æœƒæƒ³è¦å˜—è©¦å¾ã€ŒBERTã€è½‰æ›åˆ°ã€ŒXLNetã€ï¼Œåœ¨å…¶é¤˜æ¶æ§‹ä¸è®Šçš„å‰æä¸‹ï¼Œå…‰æ˜¯é€²è¡Œ Pre-train Language Model çš„æ›´æ›å¯èƒ½å°±æœƒèŠ±è²»ä¸å°‘æ™‚é–“ã€‚

é€é `Transformers` é€™å€‹å¥—ä»¶ï¼Œèƒ½å¤ å°‡è½‰æ›æˆæœ¬é™åˆ°æœ€ä½ï¼ ä½¿ç”¨é€™å¥—æ¡†æ¶èƒ½å¤ ç›´æ¥å°å¤šç¨®æ¨¡å‹é€²è¡Œæ“ä½œï¼Œå„ç¨®æ¨¡å‹çš„æ¶æ§‹åŠåƒæ•¸éƒ½å·²ç¶“è¢«å°è£åœ¨å¥—ä»¶ä¸­ï¼Œåªéœ€è¦äº†è§£æ­¤æ¡†æ¶çš„æ©Ÿåˆ¶ï¼Œå³å¯å¿«é€Ÿå¥—ç”¨åˆ°æ•¸åç¨®ä¸åŒçš„æ¨¡å‹ï¼

> ç›´ç™½çš„èªªï¼šå­¸ç¿’é€™å€‹å¥—ä»¶ï¼Œå°±èƒ½åŒæ™‚å­¸æœƒä½¿ç”¨å¤šç¨®ä¸»æµçš„èªè¨€æ¨¡å‹ã€‚

é™„ä¸Š `Transfomers` çš„ å®˜æ–¹ç¶²ç«™:

[Hugging Face - Transformers](https://huggingface.co/transformers/)

ä»¥åŠ `Transformers` ç•¶å‰æ‰€æ”¯æ´çš„æ¨¡å‹æ¸…å–®:

[Hugging Face - On a mission to solve NLP, one commit at a time.](https://huggingface.co/models)

---
<p id="å¦‚ä½•ä½¿ç”¨transformers"></p>
# å¦‚ä½•ä½¿ç”¨Transformers

æˆ‘å°‡ `Transformers` å¥—ä»¶åˆ†æˆå…©å€‹éƒ¨åˆ†: 

1. **Tokenizer**  : å°‡æ–‡å­—æ–·è©å¾Œè½‰æ›ç‚º Index. 
2. **Pre-trained Model** :  æ¥å—è½‰æ›çš„ Indexï¼Œ è¼¸å‡º Word Representation.

ä¸‹é¢é€™å¼µæµç¨‹åœ–ï¼Œæ˜¯å¾æ–‡å­—è½‰æ›æˆ Word Representationçš„éç¨‹ï¼Œæˆ‘å€‘è—‰æ­¤ä¾†äº†è§£é€™å…©å€‹å…ƒä»¶çš„ä½œç”¨(åé»ƒ)åˆ°åº•æ˜¯ä»€éº¼ :
<img style="display:block; margin-left:auto; margin-right:auto; width:100%;" src="https://minglunwu.github.io/images/20200618/word2vec_flow-2.png">

<p id="tokenizer"></p>
## Tokenizer

<p id="1-åˆå§‹åŒ–"></p>
### 1. åˆå§‹åŒ–

Tokenizer çš„åŠŸç”¨æ˜¯å°‡æ–‡å­—é€²è¡Œæ–·è©åŠè½‰æ›ç‚ºIndexï¼Œä¸åŒçš„ Pre-trained Language Model æ‰€ä½¿ç”¨çš„æ–·è©æ–¹å¼ä»¥åŠ Index ä¹Ÿä¸ç›¡ç›¸åŒï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨å‰éœ€è¦å…ˆé€²è¡Œã€Œåˆå§‹åŒ–ã€ã€‚

    :::python
    from transformers import AutoTokenizer, BertTokenizer
    
    # å¸¸è¦‹æ¨¡å‹å…·æœ‰è‡ªå·±çš„Tokenizer
    tokenizer_bert = BertTokenizer.from_pretrained("bert-base-cased")

    # AutoTokenizer å‰‡æ˜¯é€šç”¨å‹
    tokenizer_other = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")


é‡å°è¼ƒå¸¸è¦‹çš„èªè¨€æ¨¡å‹ (BERT, GPT2, XLM, XLNet...)ï¼Œ `Transformers`æœ‰æä¾›å°ˆå±¬çš„ Tokenizerå¯ä»¥ä½¿ç”¨ã€‚ 

é™¤æ­¤ä¹‹å¤–ï¼Œé‚„æœ‰é¡å¤–å®šç¾©ä¸€å€‹ `AutoTokenizer`ï¼Œå¯ä»¥è®“ä½¿ç”¨è€…äº’ç›¸åˆ†äº«ã€ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ã€‚  é€é `tokenizer.from_pretrained(<model_name>)`å³å¯ä¸‹è¼‰ä¸¦ä¸”è¼‰å…¥è©²æ¨¡å‹çš„ç›¸é—œè¨­å®šã€‚

åœ¨åˆæ¬¡ä½¿ç”¨æ¨¡å‹æ™‚ï¼Œ `transformers`æœƒè‡ªå‹•ä¸‹è¼‰è©²æ¨¡å‹çš„æ¬Šé‡åŠç›¸é—œè¨­å®šï¼Œéœ€è¦èŠ±è²»ä¸€é»æ™‚é–“ (æ™‚é–“é•·åº¦ç«¯çœ‹æ¨¡å‹çš„å¤§å°)ï¼Œä¸‹è¼‰å®Œæˆå¾Œï¼Œå¾€å¾Œçš„åŸ·è¡Œå°‡æœƒå¾æœ¬åœ°ç«¯çš„å¿«å–è³‡æ–™å¤¾è¼‰å…¥ï¼Œå°±ä¸éœ€è¦å†é€²è¡Œä¸‹è¼‰äº†ã€‚

<p id="2-åŸºæœ¬ä½¿ç”¨"></p>
### 2. åŸºæœ¬ä½¿ç”¨

    :::python
    tokenizer_bert = BertTokenizer.from_pretrained("bert-base-cased")
    test_input = "I don't want to work."

    encode_result = tokenizer.encode(test_input)  # ä½¿ç”¨ encode() ä¾†å°æ–‡å­—é€²è¡Œæ–·è©ã€ç·¨ç¢¼ã€‚
    print(encode_result)
    # [101, 146, 1274, 112, 189, 1328, 1106, 1250, 119, 102]

    decode_result = tokenizer.decode(encode_result) # ä½¿ç”¨ decode() ä¾†å°‡ id è½‰æ›å›æ–‡å­—ã€‚
    print(decode_result)
    # "[CLS] I don't want to work. [SEP]"
    

å¾ä¸­æˆ‘å€‘è§€å¯Ÿåˆ°å…©ä»¶äº‹æƒ…ï¼š
    
 1. `encode_result` çš„é•·åº¦å¥½åƒè·Ÿ `test_input` çš„é•·åº¦ä¸ç›¸åŒï¼: 
   
     é€™æ˜¯å› ç‚ºæœ‰äº›æ¨¡å‹æœƒä½¿ç”¨ Word Piece Tokenizerï¼Œä¹Ÿå°±æ˜¯åœ¨æ–·è©éç¨‹ä¸­å°‡è©å½™é€²è¡Œæ‹†è§£ï¼Œæ‰€ä»¥æœƒå°è‡´Encodeéçš„çµæœé•·åº¦èˆ‡åŸå§‹Inputä¸åŒï¼

 2. Decodeå¾Œçš„çµæœå¥½åƒè·ŸåŸæœ¬çš„ä¸å¤ªä¸€æ¨£...: 
   
     å› ç‚ºåœ¨ `tokenizer.encode()`éç¨‹ä¸­æœƒè‡ªå‹•åŠ å…¥ [CLS]ã€[SEP]ç­‰ç‰¹æ®Šæ¨™è¨˜ï¼Œé€™æ˜¯ç”±æ–¼ BERT æ¨¡å‹çš„æ©Ÿåˆ¶è€Œè‡ªå‹•ç”¢ç”Ÿï¼Œå…¶é¤˜æ¨¡å‹çš„ Tokenizer ä¸ä¸€å®šæœƒæœ‰ï¼Œä¹Ÿå¯ä»¥åœ¨encodeéç¨‹ä¸­åŠ å…¥åƒæ•¸ä¾†ç§»é™¤:

        :::python
        encode_result = tokenizer.encode(test_input, add_special_tokens=False) # ç·¨ç¢¼éç¨‹ä¸­ï¼Œç§»é™¤ç‰¹æ®Šç¬¦è™Ÿ
        print(encode_result) 
        # [146, 1274, 112, 189, 1328, 1106, 1250, 119]

        decode_result = tokenizer.decode(encode_result) # ä½¿ç”¨ decode() ä¾†å°‡ id è½‰æ›å›æ–‡å­—ã€‚
        print(decode_result)
        # "I don't want to work."


é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨é€²è¡Œè³‡æ–™å‰è™•ç†æ™‚ï¼Œå¦å¤–ä¸€å€‹éº»ç…©çš„å•é¡Œæ˜¯åœ¨ç”¢ç”Ÿ "batch"æ™‚ï¼Œå¿…é ˆè¦è®“æ¯ä¸€ç­†è³‡æ–™çš„é•·åº¦ç›¸åŒ(ä¹Ÿå°±æ˜¯è¦é€²è¡Œ"Padding")ï¼Œå‡è¨­æœ€å¤§é•·åº¦è¨­å®šç‚º `64`: 

- é•·åº¦ä¸è¶³çš„å¥å­éœ€è¦è£œé½Š "[PAD]"ç¬¦è™Ÿï¼Œä½¿é•·åº¦é”åˆ° 64ã€‚
- é•·åº¦éé•·çš„å¥å­éœ€è¦åªä¿ç•™å‰64å€‹å­—ï¼Œè¶…éçš„éƒ¨åˆ†æ¨æ£„ä¸ç”¨ã€‚

é€™ä»¶äº‹æƒ…å¦‚æœè‡ªå·±è™•ç†ï¼Œæœƒæœ‰é»å°éº»ç…©ï¼Œå¹¸å¥½ Tokenizer éƒ½å¹«å¿™åšå¥½äº†.

    :::python
    test_input = "I don't want to work."

    encode_result = tokenizer.encode(test_input, max_length=64, pad_to_max_length=True)
    print(encode_result)
    # [101, 146, 1274, 112, 189, 1328, 1106, 1250, 119, 102, 0, 0, 0,.... 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

    decode_result = tokenizer.decode(encode_result)
    print(decode_result)
    # "[CLS] I don't want to work. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]...[PAD]"
    

é€é Tokenizer é€²è¡Œæ–·è©åŠç·¨ç¢¼å¾Œï¼Œæˆ‘å€‘å°±èƒ½å°‡å…¶é€å…¥ Pre-trained Model ä¸­å–å¾— Representationäº†ï¼

---

<p id="pre-trained-language-model"></p>
## Pre-trained Language Model

<p id="1-åˆå§‹åŒ–-1"></p>
### 1. åˆå§‹åŒ–

    :::python
    from transformers import AutoModel, BertModel

    model_bert = BertModel.from_pretrained("bert-base-cased")
    model_other = AutoModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
    

èˆ‡ `Tokenizer`çš„æ©Ÿåˆ¶ç›¸åŒï¼Œ `Transformers`ä¸­ä¹ŸåŒ…å«äº†å„ç¨®é å…ˆå®šç¾©å¥½çš„ Model (ä¾‹å¦‚ `BertModel`)ï¼Œä»¥åŠæ–¹ä¾¿ä½¿ç”¨å…¶ä»–é–‹æºæ¨¡å‹çš„æ³›ç”¨Model (`AutoModel`)ã€‚

åœ¨ä½¿ç”¨å‰ä¹Ÿéœ€è¦é€é `model.from_pretrained(<model_name>)`ä¾†æŒ‡å®šè¦è¼‰å…¥çš„æ¨¡å‹ã€‚

èˆ‡ `Tokenizer`ç›¸åŒï¼Œåˆæ¬¡ä½¿ç”¨æ¨¡å‹æ™‚æœƒéœ€è¦é€²è¡Œä¸‹è¼‰ï¼Œæ‰€ä»¥éœ€è¦ç­‰å¾…ä¸€æ®µæ™‚é–“ï¼Œä»¥`xlnet-large-cased`ç‚ºä¾‹ï¼Œå¤§ç´„æœ‰3.3Géœ€è¦é€²è¡Œä¸‹è¼‰ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼š`Tokenizer` åŠ `Model` æ˜¯æœ‰å…ˆå¾Œé—œä¿‚çš„ï¼š
1. æ–‡å­—é€²å…¥`Tokenizer`è½‰æ›ã€‚
2. è½‰æ›å¾Œçš„Indexé€å…¥`Model`ä¸­å–å¾—Word Representationã€‚
 
æ‰€ä»¥ `Tokenizer` åŠ `Model` çš„ `<model_name>`å¿…éœ€è¦ä¸€è‡´ï¼Œä»¥å…ç™¼ç”ŸéŒ¯èª¤ã€‚

<p id="2-åŸºæœ¬ä½¿ç”¨-1"></p>
### 2. åŸºæœ¬ä½¿ç”¨

    :::python
    tokenizer_bert = BertTokenizer.from_pretrained("bert-base-cased")
    test_input = "I don't want to work."

    encode_result = tokenizer.encode(test_input, max_length=64, pad_to_max_length=True)  # ä½¿ç”¨ encode() ä¾†å°æ–‡å­—é€²è¡Œæ–·è©ã€ç·¨ç¢¼ã€‚
    print(encode_result)
    # [101, 146, 1274, 112, 189, 1328, 1106, 1250, 119, 102, 0, 0, ..., 0]

    model_bert = BertModel.from_pretrained("bert-base-cased")

    output = model_bert(encode_result) # ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.
    last_hidden_layer = output[0] # é€™å°±æ˜¯ â€œtest_input" çš„ word representation.

    print(last_hidden_layer.shape)
    # (1, 64, 768)   => (batch, words, word vector dimension)
    

`transformers`ä¸­çš„æ‰€æœ‰ `model` éƒ½æ˜¯ç¹¼æ‰¿ `torch.nn.Module`è€Œä¾†ï¼Œåœ¨ä½¿ç”¨ä¸Šå…¶å¯¦è·Ÿ Pytorch çš„ Model ç›¸ç•¶é¡ä¼¼ï¼ æ‰€ä»¥åœ¨ä¸Šè¿°çš„èªæ³•ä¸­å°‡ `encode_result` é€å…¥ `model_bert`ä¸­ï¼Œå°±ç­‰åŒæ–¼å°‡ `encode_result`é€å…¥ `model` ä¸­é€²è¡Œ Forward Propagationï¼Œæœ€å¾Œå³å¯å¾—åˆ° Word Representation.

è‡³æ–¼ç‚ºä»€éº¼ `last_hidden_layer = output[0]`ï¼Œå¯ä»¥åƒè€ƒ BertModel çš„ Return æ¬„ä½ï¼Œå›å‚³çš„çµæœæœ‰è¨±å¤šå…§å®¹ï¼Œè€Œç¬¬ä¸€å€‹å°±æ˜¯ã€Œæœ€å¾Œä¸€å±¤çš„ Hidden Statesã€ä¹Ÿå°±æ˜¯æˆ‘å€‘è¦çš„ Word Representation.

é™„ä¸Š `BertModel`çš„å®˜æ–¹æ–‡ä»¶é€£çµï¼š
[BERT - transformers 2.11.0 documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel.forward)

---

<p id="transformers-çš„å„ªå‹¢"></p>
# Transformers çš„å„ªå‹¢

äº†è§£ `Transformers`çš„åŸºæœ¬é‹ä½œåŸç†å¾Œï¼Œå†å›é ­å¼·èª¿ä¸€æ¬¡å®ƒçš„å„ªå‹¢ï¼: 

1. **å¯è¦–ç‚ºå°è£å¥½çš„ Language Model :**

    Transformers çš„ Model ç¹¼æ‰¿äº† `torch.nn.Module`ï¼Œä¹Ÿå°±æ˜¯èªªåœ¨è¨­è¨ˆæ•´å€‹ä»»å‹™çš„éç¨‹ä¸­ï¼Œä¸éœ€è¦è‡ªå·±å»å®šç¾© "Pre-trained Language Model" çš„çµæ§‹ã€ä¹Ÿä¸éœ€è¦è‡ªè¡Œè¼‰å…¥æ¬Šé‡ï¼Œ**åªéœ€è¦å°‡å…¶è¦–ç‚ºä¸€å€‹Pytorch é å…ˆå®šç¾©å¥½çš„ Model ï¼š**å¯¦ä½œä¸€å€‹ `Transformers`çš„Modelã€è¼‰å…¥ç›¸é—œæ¬Šé‡ï¼Œæ¥ä¸‹ä¾†åªéœ€è¦çµ„è£å°±å¥½äº†ï¼

        :::python
        model_bert = BertModel.from_pretrained("bert-base-cased")
        model_ln1 = nn.Linear(768, 3) 

        output = model_bert(encode_result) # ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.
        last_hidden_layer = output[0]
        output = model_ln1(last_hidden_layer) # å¯ä»¥ç›´æ¥å†å°‡ BertModel çš„ Outputé€å…¥å…¶ä»–è‡ªå®šç¾©çš„çµæ§‹ã€‚
        

    äº†è§£åˆ°å¦‚ä½•çµ„è£ã€å¦‚ä½•ä½¿ç”¨å¾Œï¼Œé™¤äº†å°‡å…¶æ‹¿ä¾†åš Feature Extractionå¤–ï¼Œä¹Ÿå¯ä»¥ç”¨ç›¸åŒçš„æ¦‚å¿µçµ„è£æˆ Transfer Learning çš„æ¨¡å‹æ¶æ§‹ï¼ä¸éé€™è¶…å‡ºé€™å‰‡ç­†è¨˜æƒ³è¦ç´€éŒ„çš„ç¯„åœäº†ï½

    å¦å¤–ï¼Œç¿’æ…£ä½¿ç”¨ Tensorflow çš„æœ‹å‹ä¹Ÿåˆ¥å‚·å¿ƒï¼Œ `transformers`ä¹Ÿæœ‰æä¾› tensorflowç‰ˆæœ¬çš„ Model (ä¾‹å¦‚ `tfBertModel`)!

2. **åŒæ¨£çš„æ¶æ§‹èƒ½ä½¿ç”¨å¤šç¨®æ¨¡å‹ â†’ æ–¹ä¾¿æŠ½æ›**

    ç”±æ–¼ `Transformers` ä¸­åŒ…å«è¨±å¤šæ¨¡å‹ï¼šæœ‰éƒ¨åˆ†æ˜¯å®˜æ–¹æŒçºŒæ–°å¢ï¼Œæœ‰äº›å‰‡æ˜¯ä½¿ç”¨è€…äº’ç›¸é‡‹å‡ºï¼Œå°è‡´ä½¿ç”¨é€™å¥—æ¶æ§‹å°±èƒ½å¿«é€Ÿåˆ‡æ›å¤šç¨®æ¨¡å‹ã€‚

        :::python
        MODEL = "bert-base-cased"
        model_language = AutoModel.from_pretrained(MODEL)
        model_ln1 = nn.Linear(768, 3) 

        output = model_language(encode_result) # ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.
        last_hidden_layer = output[0]
        output = model_ln1(last_hidden_layer) # å¯ä»¥ç›´æ¥å†å°‡ model_language çš„ Outputé€å…¥å…¶ä»–è‡ªå®šç¾©çš„çµæ§‹ã€‚
        

    ç›¸åŒçš„ç¨‹å¼ç¢¼ï¼Œåªéœ€è¦æ›´æ›æ¨¡å‹è®Šæ•¸å³å¯é”åˆ°ã€ŒæŠ½æ›åº•å±¤ Pre-trained Langauge Modelçš„æ•ˆæœã€

        :::python
        MODEL = "xlnet-large-cased"
        model_language = AutoModel.from_pretrained(MODEL)
        model_ln1 = nn.Linear(768, 3) 

        output = model_language(encode_result) # ç­‰åŒæ–¼å°‡ encode_result é€å…¥ model ä¸­é€²è¡Œ forward propagation.
        last_hidden_layer = output[0]
        output = model_ln1(last_hidden_layer) # å¯ä»¥ç›´æ¥å†å°‡ model_language çš„ Outputé€å…¥å…¶ä»–è‡ªå®šç¾©çš„çµæ§‹ã€‚
        

---

<p id="å¾Œè¨˜"></p>
# å¾Œè¨˜
äº†è§£ `Transformers` é€™å€‹å¥—ä»¶å¾Œï¼Œåœ¨é€²è¡Œ NLP çš„ç›¸é—œä»»å‹™ä¸Šç¯€çœäº†è¨±å¤šæ™‚é–“ï¼Œå¯ä»¥å¿«é€Ÿåœ°åˆ‡æ›Pre-trained Language Model ä¾†é€²è¡Œå„ç¨®å˜—è©¦ã€‚

æ­¤å¤–ï¼Œåœ¨Surveyæ–°çš„Language Modelæ™‚ï¼Œå¦‚æœåœ¨Githubä¸Šé¢çœ‹åˆ°å®ƒæ”¯æ´ `transformers`ï¼Œæœƒå‚™æ„Ÿæ¬£æ…°QQï¼Œé “æ™‚æ„Ÿåˆ°ä¸–ç•Œçš„ç¾å¥½ï½

æ„Ÿè¬ä½ çš„é–±è®€ï¼Œå¸Œæœ›ä½ ä¹Ÿèƒ½è·Ÿæˆ‘ä¸€èµ·æ„Ÿå—ä¸–ç•Œçš„ç¾å¥½ï¼XD 

æœ‰ä»»ä½•å•é¡Œæ­¡è¿ä¸€èµ·äº¤æµï¼ ä¸‹æ¬¡è¦‹ï¼